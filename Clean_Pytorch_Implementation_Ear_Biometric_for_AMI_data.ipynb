{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shujaat123/Ear_Biometrics/blob/main/Clean_Pytorch_Implementation_Ear_Biometric_for_AMI_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57mAz8uQMn9G",
        "outputId": "bd9a5632-4bc5-4d33-e3aa-e9c6b4cdeb44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9656 sha256=788257d88e8382b76162ae61e7481dbc2fd0ca378dcf3cb8bd33de5a891e2771\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n",
            "Collecting py7zr\n",
            "  Downloading py7zr-0.20.5-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable (from py7zr)\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.6.6 (from py7zr)\n",
            "  Downloading pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.14.4 (from py7zr)\n",
            "  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1 (from py7zr)\n",
            "  Downloading pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.8/138.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybcj>=0.6.0 (from py7zr)\n",
            "  Downloading pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3 (from py7zr)\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting brotli>=1.0.9 (from py7zr)\n",
            "  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflate64>=0.3.1 (from py7zr)\n",
            "  Downloading inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from py7zr) (5.9.5)\n",
            "Installing collected packages: texttable, brotli, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, py7zr\n",
            "Successfully installed brotli-1.0.9 inflate64-0.3.1 multivolumefile-0.2.3 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 texttable-1.6.7\n"
          ]
        }
      ],
      "source": [
        "## Load useful packages\n",
        "!pip install wget\n",
        "!pip install py7zr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import py7zr\n",
        "from zipfile import ZipFile\n",
        "from random import sample\n",
        "import PIL.Image as Image\n",
        "import matplotlib.pyplot as plt\n",
        "from  sklearn.model_selection import train_test_split\n",
        "from os import listdir\n",
        "from os import path\n",
        "import h5py\n",
        "import numpy as np\n",
        "import wget"
      ],
      "metadata": {
        "id": "WaA0QTb9MwPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING IITD Dataset\n",
        "data_path = 'https://github.com/Shujaat123/Ear_Biometrics/blob/main/datasets/IITD_Dataset.7z?raw=true'\n",
        "filename = 'IITD_Dataset.7z'\n",
        "if(path.exists(filename)):\n",
        "  !rm $filename\n",
        "  print('existing file:', filename, ' has been deleted')\n",
        "print('downloading latest version of file:', filename)\n",
        "wget.download(data_path, filename)\n",
        "print('DONE')\n",
        "\n",
        "with py7zr.SevenZipFile('IITD_Dataset.7z', mode='r') as z:\n",
        "    z.extractall()\n",
        "!ls\n",
        "\n",
        "# Processing IITD_dataset\n",
        "src_dir = 'ear/processed/221'\n",
        "images_name = listdir(src_dir)\n",
        "images_name_temp = []\n",
        "subjects = []\n",
        "for img_ind in range(0,len(images_name)):\n",
        "  if(not(images_name[img_ind]=='Thumbs.db')):\n",
        "    subjects.append(int(images_name[img_ind].split('_')[0]))\n",
        "    images_name_temp.append(images_name[img_ind])\n",
        "\n",
        "images_name = images_name_temp\n",
        "images_name_ord = []\n",
        "subjects_ord = []\n",
        "\n",
        "sub_ind = sorted(range(len(subjects)),key=subjects.__getitem__)\n",
        "for pos, item in enumerate(sub_ind):\n",
        "  images_name_ord.append(images_name[item])\n",
        "  subjects_ord.append(subjects[item])\n",
        "\n",
        "images_name = images_name_ord\n",
        "subjects = subjects_ord\n",
        "\n",
        "print(subjects)\n",
        "print(images_name)\n",
        "\n",
        "img_ind = 0\n",
        "ear_images = []\n",
        "sub_labels = [];\n",
        "target_size = (180, 50)\n",
        "\n",
        "for sub_ind in range(0,len(subjects)):\n",
        "  img_path = src_dir+'/'+images_name[sub_ind]\n",
        "  ear_img = (plt.imread(img_path))/255\n",
        "\n",
        "  ear_img = Image.open(img_path)\n",
        "  ear_img = ear_img.resize(target_size, Image.ANTIALIAS)\n",
        "  ear_img = np.asarray(ear_img).astype(np.float32)/255\n",
        "\n",
        "  ear_images.append(ear_img)\n",
        "  sub_labels.append(subjects[sub_ind]-1)\n",
        "\n",
        "ear_images = np.array(ear_images)\n",
        "sub_labels = np.array(sub_labels)\n",
        "\n",
        "print(ear_images.shape)\n",
        "print(sub_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-gSS5XGlpPy",
        "outputId": "151cd6bd-9cd6-481b-e823-6e00cd96c2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading latest version of file: IITD_Dataset.7z\n",
            "DONE\n",
            "ear  IITD_Dataset.7z  sample_data\n",
            "[1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 10, 11, 11, 11, 12, 12, 12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20, 21, 21, 21, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 24, 25, 25, 25, 25, 26, 26, 26, 27, 27, 27, 27, 27, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 32, 32, 32, 32, 33, 33, 33, 34, 34, 34, 35, 35, 35, 35, 36, 36, 36, 37, 37, 37, 37, 38, 38, 38, 39, 39, 39, 40, 40, 40, 41, 41, 41, 41, 42, 42, 42, 42, 43, 43, 43, 43, 43, 44, 44, 44, 45, 45, 45, 46, 46, 46, 46, 47, 47, 47, 48, 48, 48, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 51, 51, 51, 52, 52, 52, 52, 53, 53, 53, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 56, 56, 56, 56, 57, 57, 57, 57, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 60, 60, 60, 61, 61, 61, 62, 62, 62, 63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 65, 65, 65, 66, 66, 66, 66, 67, 67, 67, 68, 68, 68, 68, 69, 69, 69, 69, 69, 70, 70, 70, 70, 71, 71, 71, 72, 72, 72, 73, 73, 73, 74, 74, 74, 75, 75, 75, 76, 76, 76, 76, 76, 76, 77, 77, 77, 78, 78, 78, 79, 79, 79, 80, 80, 80, 80, 80, 81, 81, 81, 81, 82, 82, 82, 82, 83, 83, 83, 83, 84, 84, 84, 85, 85, 85, 85, 85, 85, 86, 86, 86, 86, 86, 87, 87, 87, 87, 88, 88, 88, 88, 88, 89, 89, 89, 89, 90, 90, 90, 90, 91, 91, 91, 91, 91, 91, 92, 92, 92, 93, 93, 93, 93, 93, 93, 94, 94, 94, 94, 95, 95, 95, 95, 96, 96, 96, 96, 96, 96, 97, 97, 97, 98, 98, 98, 98, 99, 99, 99, 99, 99, 99, 100, 100, 100, 101, 101, 101, 101, 101, 101, 102, 102, 102, 102, 102, 103, 103, 103, 103, 104, 104, 104, 104, 105, 105, 105, 106, 106, 106, 107, 107, 107, 107, 108, 108, 108, 108, 109, 109, 109, 110, 110, 110, 111, 111, 111, 112, 112, 112, 112, 113, 113, 113, 114, 114, 114, 114, 114, 115, 115, 115, 115, 116, 116, 116, 116, 116, 117, 117, 117, 117, 118, 118, 118, 119, 119, 119, 119, 120, 120, 120, 120, 121, 121, 121, 122, 122, 122, 123, 123, 123, 124, 124, 124, 125, 125, 125, 126, 126, 126, 127, 127, 127, 128, 128, 128, 129, 129, 129, 130, 130, 130, 131, 131, 131, 131, 132, 132, 132, 133, 133, 133, 134, 134, 134, 135, 135, 135, 136, 136, 136, 137, 137, 137, 138, 138, 138, 139, 139, 139, 140, 140, 140, 141, 141, 141, 141, 142, 142, 142, 143, 143, 143, 144, 144, 144, 145, 145, 145, 146, 146, 146, 147, 147, 147, 148, 148, 148, 149, 149, 149, 150, 150, 150, 151, 151, 151, 152, 152, 152, 153, 153, 153, 154, 154, 154, 155, 155, 155, 156, 156, 156, 157, 157, 157, 158, 158, 158, 159, 159, 159, 160, 160, 160, 161, 161, 161, 162, 162, 162, 163, 163, 163, 164, 164, 164, 165, 165, 165, 166, 166, 166, 167, 167, 167, 168, 168, 168, 169, 169, 169, 170, 170, 170, 171, 171, 171, 172, 172, 172, 173, 173, 173, 174, 174, 174, 175, 175, 175, 176, 176, 176, 177, 177, 177, 178, 178, 178, 179, 179, 179, 180, 180, 180, 180, 181, 181, 181, 181, 182, 182, 182, 183, 183, 183, 184, 184, 184, 185, 185, 185, 186, 186, 186, 187, 187, 187, 188, 188, 188, 189, 189, 189, 189, 190, 190, 190, 191, 191, 191, 191, 192, 192, 192, 193, 193, 193, 194, 194, 194, 195, 195, 195, 196, 196, 196, 196, 197, 197, 197, 198, 198, 198, 199, 199, 199, 200, 200, 200, 201, 201, 201, 202, 202, 202, 203, 203, 203, 203, 204, 204, 204, 205, 205, 205, 206, 206, 206, 207, 207, 207, 208, 208, 208, 209, 209, 209, 210, 210, 210, 211, 211, 211, 212, 212, 212, 212, 212, 213, 213, 213, 213, 213, 213, 214, 214, 214, 214, 214, 214, 215, 215, 215, 215, 216, 216, 216, 216, 217, 217, 217, 218, 218, 218, 218, 218, 218, 219, 219, 219, 219, 219, 220, 220, 220, 221, 221, 221]\n",
            "['001_3.bmp', '001_2.bmp', '001_5.bmp', '001_1.bmp', '001_6.bmp', '001_4.bmp', '002_3.bmp', '002_1.bmp', '002_2.bmp', '003_1.bmp', '003_4.bmp', '003_5.bmp', '003_3.bmp', '003_2.bmp', '004_6.bmp', '004_5.bmp', '004_4.bmp', '004_1.bmp', '004_2.bmp', '004_3.bmp', '005_2.bmp', '005_1.bmp', '005_4.bmp', '005_3.bmp', '006_3.bmp', '006_2.bmp', '006_1.bmp', '007_1.bmp', '007_2.bmp', '007_3.bmp', '007_4.bmp', '008_2.bmp', '008_3.bmp', '008_1.bmp', '009_2.bmp', '009_1.bmp', '009_3.bmp', '010_3.bmp', '010_1.bmp', '010_2.bmp', '011_1.bmp', '011_3.bmp', '011_2.bmp', '012_1.bmp', '012_3.bmp', '012_2.bmp', '013_3.bmp', '013_1.bmp', '013_5.bmp', '013_4.bmp', '013_6.bmp', '013_2.bmp', '014_3.bmp', '014_1.bmp', '014_2.bmp', '015_3.bmp', '015_2.bmp', '015_1.bmp', '016_3.bmp', '016_4.bmp', '016_1.bmp', '016_2.bmp', '017_3.bmp', '017_2.bmp', '017_1.bmp', '018_2.bmp', '018_3.bmp', '018_1.bmp', '019_1.bmp', '019_4.bmp', '019_2.bmp', '019_3.bmp', '020_1.bmp', '020_2.bmp', '020_3.bmp', '021_1.bmp', '021_3.bmp', '021_2.bmp', '022_2.bmp', '022_1.bmp', '022_3.bmp', '023_2.bmp', '023_1.bmp', '023_3.bmp', '023_4.bmp', '024_4.bmp', '024_1.bmp', '024_2.bmp', '024_3.bmp', '025_2.bmp', '025_4.bmp', '025_1.bmp', '025_3.bmp', '026_3.bmp', '026_1.bmp', '026_2.bmp', '027_4.bmp', '027_2.bmp', '027_5.bmp', '027_1.bmp', '027_3.bmp', '028_2.bmp', '028_1.bmp', '028_3.bmp', '029_2.bmp', '029_1.bmp', '029_3.bmp', '029_4.bmp', '030_4.bmp', '030_2.bmp', '030_1.bmp', '030_3.bmp', '031_3.bmp', '031_2.bmp', '031_1.bmp', '032_1.bmp', '032_4.bmp', '032_2.bmp', '032_3.bmp', '033_3.bmp', '033_1.bmp', '033_2.bmp', '034_3.bmp', '034_1.bmp', '034_2.bmp', '035_2.bmp', '035_4.bmp', '035_1.bmp', '035_3.bmp', '036_2.bmp', '036_3.bmp', '036_1.bmp', '037_3.bmp', '037_4.bmp', '037_2.bmp', '037_1.bmp', '038_1.bmp', '038_2.bmp', '038_3.bmp', '039_3.bmp', '039_2.bmp', '039_1.bmp', '040_2.bmp', '040_1.bmp', '040_3.bmp', '041_2.bmp', '041_4.bmp', '041_1.bmp', '041_3.bmp', '042_1.bmp', '042_3.bmp', '042_4.bmp', '042_2.bmp', '043_2.bmp', '043_4.bmp', '043_5.bmp', '043_3.bmp', '043_1.bmp', '044_1.bmp', '044_2.bmp', '044_3.bmp', '045_1.bmp', '045_3.bmp', '045_2.bmp', '046_2.bmp', '046_3.bmp', '046_4.bmp', '046_1.bmp', '047_1.bmp', '047_2.bmp', '047_3.bmp', '048_2.bmp', '048_3.bmp', '048_1.bmp', '049_1.bmp', '049_4.bmp', '049_3.bmp', '049_5.bmp', '049_2.bmp', '050_2.bmp', '050_4.bmp', '050_1.bmp', '050_5.bmp', '050_3.bmp', '051_2.bmp', '051_1.bmp', '051_3.bmp', '052_4.bmp', '052_1.bmp', '052_2.bmp', '052_3.bmp', '053_1.bmp', '053_2.bmp', '053_3.bmp', '054_3.bmp', '054_2.bmp', '054_4.bmp', '054_5.bmp', '054_1.bmp', '055_1.bmp', '055_5.bmp', '055_3.bmp', '055_4.bmp', '055_2.bmp', '056_4.bmp', '056_2.bmp', '056_1.bmp', '056_3.bmp', '057_1.bmp', '057_3.bmp', '057_4.bmp', '057_2.bmp', '058_4.bmp', '058_3.bmp', '058_2.bmp', '058_5.bmp', '058_1.bmp', '059_3.bmp', '059_4.bmp', '059_1.bmp', '059_5.bmp', '059_2.bmp', '060_3.bmp', '060_1.bmp', '060_2.bmp', '061_2.bmp', '061_1.bmp', '061_3.bmp', '062_3.bmp', '062_2.bmp', '062_1.bmp', '063_1.bmp', '063_3.bmp', '063_2.bmp', '063_4.bmp', '063_5.bmp', '063_6.bmp', '064_1.bmp', '064_5.bmp', '064_6.bmp', '064_3.bmp', '064_4.bmp', '064_2.bmp', '065_1.bmp', '065_3.bmp', '065_2.bmp', '066_4.bmp', '066_1.bmp', '066_2.bmp', '066_3.bmp', '067_2.bmp', '067_1.bmp', '067_3.bmp', '068_4.bmp', '068_2.bmp', '068_1.bmp', '068_3.bmp', '069_5.bmp', '069_2.bmp', '069_3.bmp', '069_1.bmp', '069_4.bmp', '070_2.bmp', '070_3.bmp', '070_4.bmp', '070_1.bmp', '071_2.bmp', '071_1.bmp', '071_3.bmp', '072_1.bmp', '072_3.bmp', '072_2.bmp', '073_3.bmp', '073_2.bmp', '073_1.bmp', '074_1.bmp', '074_2.bmp', '074_3.bmp', '075_1.bmp', '075_2.bmp', '075_3.bmp', '076_5.bmp', '076_3.bmp', '076_1.bmp', '076_4.bmp', '076_6.bmp', '076_2.bmp', '077_2.bmp', '077_1.bmp', '077_3.bmp', '078_1.bmp', '078_3.bmp', '078_2.bmp', '079_3.bmp', '079_2.bmp', '079_1.bmp', '080_5.bmp', '080_2.bmp', '080_4.bmp', '080_1.bmp', '080_3.bmp', '081_4.bmp', '081_1.bmp', '081_3.bmp', '081_2.bmp', '082_4.bmp', '082_3.bmp', '082_2.bmp', '082_1.bmp', '083_2.bmp', '083_1.bmp', '083_3.bmp', '083_4.bmp', '084_2.bmp', '084_3.bmp', '084_1.bmp', '085_3.bmp', '085_4.bmp', '085_5.bmp', '085_2.bmp', '085_6.bmp', '085_1.bmp', '086_1.bmp', '086_5.bmp', '086_3.bmp', '086_2.bmp', '086_4.bmp', '087_3.bmp', '087_2.bmp', '087_4.bmp', '087_1.bmp', '088_3.bmp', '088_2.bmp', '088_1.bmp', '088_5.bmp', '088_4.bmp', '089_2.bmp', '089_1.bmp', '089_4.bmp', '089_3.bmp', '090_1.bmp', '090_3.bmp', '090_2.bmp', '090_4.bmp', '091_6.bmp', '091_2.bmp', '091_5.bmp', '091_4.bmp', '091_1.bmp', '091_3.bmp', '092_2.bmp', '092_1.bmp', '092_3.bmp', '093_1.bmp', '093_6.bmp', '093_2.bmp', '093_5.bmp', '093_3.bmp', '093_4.bmp', '094_3.bmp', '094_4.bmp', '094_1.bmp', '094_2.bmp', '095_1.bmp', '095_4.bmp', '095_2.bmp', '095_3.bmp', '096_3.bmp', '096_4.bmp', '096_5.bmp', '096_1.bmp', '096_6.bmp', '096_2.bmp', '097_3.bmp', '097_2.bmp', '097_1.bmp', '098_3.bmp', '098_1.bmp', '098_4.bmp', '098_2.bmp', '099_2.bmp', '099_1.bmp', '099_6.bmp', '099_5.bmp', '099_3.bmp', '099_4.bmp', '100_1.bmp', '100_3.bmp', '100_2.bmp', '101_6.bmp', '101_2.bmp', '101_5.bmp', '101_3.bmp', '101_4.bmp', '101_1.bmp', '102_1.bmp', '102_5.bmp', '102_3.bmp', '102_2.bmp', '102_4.bmp', '103_4.bmp', '103_3.bmp', '103_2.bmp', '103_1.bmp', '104_2.bmp', '104_4.bmp', '104_1.bmp', '104_3.bmp', '105_2.bmp', '105_3.bmp', '105_1.bmp', '106_2.bmp', '106_3.bmp', '106_1.bmp', '107_4.bmp', '107_1.bmp', '107_2.bmp', '107_3.bmp', '108_1.bmp', '108_2.bmp', '108_3.bmp', '108_4.bmp', '109_3.bmp', '109_1.bmp', '109_2.bmp', '110_3.bmp', '110_1.bmp', '110_2.bmp', '111_1.bmp', '111_2.bmp', '111_3.bmp', '112_4.bmp', '112_2.bmp', '112_1.bmp', '112_3.bmp', '113_2.bmp', '113_1.bmp', '113_3.bmp', '114_4.bmp', '114_3.bmp', '114_1.bmp', '114_2.bmp', '114_5.bmp', '115_4.bmp', '115_2.bmp', '115_1.bmp', '115_3.bmp', '116_2.bmp', '116_1.bmp', '116_5.bmp', '116_4.bmp', '116_3.bmp', '117_2.bmp', '117_1.bmp', '117_3.bmp', '117_4.bmp', '118_1.bmp', '118_2.bmp', '118_3.bmp', '119_3.bmp', '119_2.bmp', '119_1.bmp', '119_4.bmp', '120_4.bmp', '120_3.bmp', '120_2.bmp', '120_1.bmp', '121_3.bmp', '121_2.bmp', '121_1.bmp', '122_2.bmp', '122_1.bmp', '122_3.bmp', '123_1.bmp', '123_3.bmp', '123_2.bmp', '124_3.bmp', '124_2.bmp', '124_1.bmp', '125_3.bmp', '125_2.bmp', '125_1.bmp', '126_2.bmp', '126_3.bmp', '126_1.bmp', '127_1.bmp', '127_2.bmp', '127_3.bmp', '128_3.bmp', '128_2.bmp', '128_1.bmp', '129_1.bmp', '129_3.bmp', '129_2.bmp', '130_1.bmp', '130_3.bmp', '130_2.bmp', '131_2.bmp', '131_3.bmp', '131_1.bmp', '131_4.bmp', '132_1.bmp', '132_2.bmp', '132_3.bmp', '133_3.bmp', '133_2.bmp', '133_1.bmp', '134_3.bmp', '134_1.bmp', '134_2.bmp', '135_2.bmp', '135_1.bmp', '135_3.bmp', '136_2.bmp', '136_1.bmp', '136_3.bmp', '137_2.bmp', '137_3.bmp', '137_1.bmp', '138_3.bmp', '138_1.bmp', '138_2.bmp', '139_3.bmp', '139_2.bmp', '139_1.bmp', '140_3.bmp', '140_2.bmp', '140_1.bmp', '141_3.bmp', '141_1.bmp', '141_4.bmp', '141_2.bmp', '142_3.bmp', '142_2.bmp', '142_1.bmp', '143_3.bmp', '143_2.bmp', '143_1.bmp', '144_3.bmp', '144_1.bmp', '144_2.bmp', '145_2.bmp', '145_3.bmp', '145_1.bmp', '146_3.bmp', '146_2.bmp', '146_1.bmp', '147_1.bmp', '147_2.bmp', '147_3.bmp', '148_2.bmp', '148_3.bmp', '148_1.bmp', '149_3.bmp', '149_2.bmp', '149_1.bmp', '150_1.bmp', '150_3.bmp', '150_2.bmp', '151_1.bmp', '151_3.bmp', '151_2.bmp', '152_1.bmp', '152_3.bmp', '152_2.bmp', '153_3.bmp', '153_1.bmp', '153_2.bmp', '154_3.bmp', '154_2.bmp', '154_1.bmp', '155_1.bmp', '155_2.bmp', '155_3.bmp', '156_2.bmp', '156_3.bmp', '156_1.bmp', '157_3.bmp', '157_1.bmp', '157_2.bmp', '158_1.bmp', '158_2.bmp', '158_3.bmp', '159_1.bmp', '159_2.bmp', '159_3.bmp', '160_1.bmp', '160_2.bmp', '160_3.bmp', '161_3.bmp', '161_1.bmp', '161_2.bmp', '162_1.bmp', '162_3.bmp', '162_2.bmp', '163_1.bmp', '163_2.bmp', '163_3.bmp', '164_3.bmp', '164_2.bmp', '164_1.bmp', '165_1.bmp', '165_3.bmp', '165_2.bmp', '166_1.bmp', '166_2.bmp', '166_3.bmp', '167_3.bmp', '167_1.bmp', '167_2.bmp', '168_2.bmp', '168_1.bmp', '168_3.bmp', '169_3.bmp', '169_2.bmp', '169_1.bmp', '170_3.bmp', '170_2.bmp', '170_1.bmp', '171_1.bmp', '171_3.bmp', '171_2.bmp', '172_2.bmp', '172_1.bmp', '172_3.bmp', '173_2.bmp', '173_1.bmp', '173_3.bmp', '174_3.bmp', '174_2.bmp', '174_1.bmp', '175_1.bmp', '175_2.bmp', '175_3.bmp', '176_3.bmp', '176_2.bmp', '176_1.bmp', '177_1.bmp', '177_3.bmp', '177_2.bmp', '178_3.bmp', '178_2.bmp', '178_1.bmp', '179_3.bmp', '179_1.bmp', '179_2.bmp', '180_1.bmp', '180_3.bmp', '180_4.bmp', '180_2.bmp', '181_3.bmp', '181_2.bmp', '181_1.bmp', '181_4.bmp', '182_1.bmp', '182_2.bmp', '182_3.bmp', '183_2.bmp', '183_1.bmp', '183_3.bmp', '184_1.bmp', '184_3.bmp', '184_2.bmp', '185_1.bmp', '185_2.bmp', '185_3.bmp', '186_2.bmp', '186_1.bmp', '186_3.bmp', '187_1.bmp', '187_2.bmp', '187_3.bmp', '188_2.bmp', '188_3.bmp', '188_1.bmp', '189_4.bmp', '189_3.bmp', '189_1.bmp', '189_2.bmp', '190_3.bmp', '190_2.bmp', '190_1.bmp', '191_3.bmp', '191_1.bmp', '191_2.bmp', '191_4.bmp', '192_2.bmp', '192_3.bmp', '192_1.bmp', '193_2.bmp', '193_3.bmp', '193_1.bmp', '194_2.bmp', '194_1.bmp', '194_3.bmp', '195_2.bmp', '195_3.bmp', '195_1.bmp', '196_4.bmp', '196_2.bmp', '196_3.bmp', '196_1.bmp', '197_3.bmp', '197_1.bmp', '197_2.bmp', '198_2.bmp', '198_1.bmp', '198_3.bmp', '199_1.bmp', '199_3.bmp', '199_2.bmp', '200_1.bmp', '200_2.bmp', '200_3.bmp', '201_1.bmp', '201_3.bmp', '201_2.bmp', '202_3.bmp', '202_2.bmp', '202_1.bmp', '203_3.bmp', '203_2.bmp', '203_4.bmp', '203_1.bmp', '204_3.bmp', '204_2.bmp', '204_1.bmp', '205_2.bmp', '205_1.bmp', '205_3.bmp', '206_2.bmp', '206_3.bmp', '206_1.bmp', '207_3.bmp', '207_1.bmp', '207_2.bmp', '208_3.bmp', '208_1.bmp', '208_2.bmp', '209_3.bmp', '209_1.bmp', '209_2.bmp', '210_1.bmp', '210_3.bmp', '210_2.bmp', '211_1.bmp', '211_3.bmp', '211_2.bmp', '212_5.bmp', '212_4.bmp', '212_1.bmp', '212_3.bmp', '212_2.bmp', '213_3.bmp', '213_5.bmp', '213_1.bmp', '213_6.bmp', '213_2.bmp', '213_4.bmp', '214_6.bmp', '214_5.bmp', '214_3.bmp', '214_1.bmp', '214_2.bmp', '214_4.bmp', '215_1.bmp', '215_3.bmp', '215_2.bmp', '215_4.bmp', '216_1.bmp', '216_2.bmp', '216_3.bmp', '216_4.bmp', '217_3.bmp', '217_1.bmp', '217_2.bmp', '218_3.bmp', '218_6.bmp', '218_4.bmp', '218_1.bmp', '218_2.bmp', '218_5.bmp', '219_4.bmp', '219_2.bmp', '219_5.bmp', '219_1.bmp', '219_3.bmp', '220_3.bmp', '220_2.bmp', '220_1.bmp', '221_1.bmp', '221_3.bmp', '221_2.bmp']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c13cf143b358>:50: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  ear_img = ear_img.resize(target_size, Image.ANTIALIAS)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(793, 50, 180)\n",
            "(793,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(ear_images, sub_labels, test_size=0.382093316519, random_state=42, stratify=sub_labels)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(ear_images, sub_labels, test_size=0.2786885245901639, random_state=42, stratify=sub_labels)\n",
        "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.386363636363, random_state=42, stratify=y_train)\n",
        "\n",
        "print('Training dataset:\\n',X_train.shape)\n",
        "print(y_train.shape)\n",
        "# print('Validation dataset:\\n',X_valid.shape)\n",
        "# print(y_valid.shape)\n",
        "print('Test dataset:\\n',X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxumLS3QOES3",
        "outputId": "76bb4662-bd21-4b63-d538-53d2c3f4b42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset:\n",
            " (490, 50, 180)\n",
            "(490,)\n",
            "Test dataset:\n",
            " (303, 50, 180)\n",
            "(303,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conventional convolution"
      ],
      "metadata": {
        "id": "-Iqa_Mc5xUiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn\n",
        "import torch.nn.functional\n",
        "import torch.optim\n",
        "from torchvision import models #just for debugging"
      ],
      "metadata": {
        "id": "VbDuNIPfRIH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pytorch_BUS_Final_Model_C1(torch.nn.Module):\n",
        "  #  Determine what layers and their order in CNN object\n",
        "  def __init__(self, num_classes=221, num_filters=8, input_shape=(180,50,1)):\n",
        "    super(Pytorch_BUS_Final_Model_C1,self).__init__()\n",
        "    #self.encoder_input = input_shape[-1]\n",
        "    kernel_size = 3\n",
        "    # Encoder Layer1\n",
        "    self.encoder_layer1_name = 'encoder_layer1'\n",
        "    self.encoder_layer1_conv = torch.nn.Conv2d(1,\n",
        "                                               num_filters,\n",
        "                                               kernel_size,\n",
        "                                               padding='same')\n",
        "\n",
        "    self.encoder_layer1_activation = torch.nn.ReLU()\n",
        "    self.encoder_layer1_pooling = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "    # Encoder Layer2\n",
        "    self.encoder_layer2_name = 'encoder_layer2'\n",
        "    self.encoder_layer2_conv = torch.nn.Conv2d(num_filters,\n",
        "                                               2*num_filters,\n",
        "                                               kernel_size,\n",
        "                                               padding='same')\n",
        "    self.encoder_layer2_activation = torch.nn.ReLU()\n",
        "    self.encoder_layer2_batch_norm = torch.nn.BatchNorm2d(2*num_filters,\n",
        "                                                          eps = 1e-3,\n",
        "                                                          momentum = 0.99)\n",
        "    self.encoder_layer2_pooling = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "    # Encoder Layer3\n",
        "    self.encoder_layer3_name = 'encoder_layer3'\n",
        "    self.encoder_layer3_conv = torch.nn.Conv2d(2*num_filters,\n",
        "                                               4*num_filters,\n",
        "                                               kernel_size,\n",
        "                                               padding='same')\n",
        "    self.encoder_layer3_activation = torch.nn.ReLU()\n",
        "    self.encoder_layer3_pooling = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "    # Encoder Layer4\n",
        "    self.encoder_layer4_name = 'encoder_layer4'\n",
        "    self.encoder_layer4_conv = torch.nn.Conv2d(4*num_filters,\n",
        "                                               8*num_filters,\n",
        "                                               kernel_size,\n",
        "                                               padding='same')\n",
        "    self.encoder_layer4_activation = torch.nn.ReLU()\n",
        "    self.encoder_layer4_batch_norm = torch.nn.BatchNorm2d(8*num_filters,\n",
        "                                                          eps = 1e-3,\n",
        "                                                          momentum = 0.99)\n",
        "    self.encoder_layer4_pooling = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "    # Encoder Layer5\n",
        "    self.encoder_layer5_name = 'encoder_layer5'\n",
        "    self.encoder_layer5_conv = torch.nn.Conv2d(8*num_filters,\n",
        "                                               16*num_filters,\n",
        "                                               kernel_size,\n",
        "                                               padding='same')\n",
        "\n",
        "    self.encoder_layer5_activation = torch.nn.ReLU()\n",
        "    self.encoder_layer5_pooling = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "   # Encoder Layer6\n",
        "    self.encoder_layer6_name = 'encoder_layer2'\n",
        "    self.encoder_layer6_conv = torch.nn.Conv2d(16*num_filters,\n",
        "                                               32*num_filters,\n",
        "                                               kernel_size,\n",
        "                                               padding='same')\n",
        "    self.encoder_layer6_activation = torch.nn.ReLU()\n",
        "    self.encoder_layer6_batch_norm = torch.nn.BatchNorm2d(32*num_filters,\n",
        "                                                          eps = 1e-3,\n",
        "                                                          momentum = 0.99)\n",
        "    # Dense layer\n",
        "    self.fc1_flatten = torch.nn.Flatten()\n",
        "    self.fc1_linear = torch.nn.Linear(32*num_filters*(input_shape[0]//(2**5))*(input_shape[1]//(2**5)), num_classes)\n",
        "    self.fc1_activation = torch.nn.Softmax()\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Encoder Layer1\n",
        "    out = self.encoder_layer1_conv(x)\n",
        "    out = self.encoder_layer1_activation(out)\n",
        "    out = self.encoder_layer1_pooling(out)\n",
        "\n",
        "    # Encoder Layer2\n",
        "    out = self.encoder_layer2_conv(out)\n",
        "    out = self.encoder_layer2_activation(out)\n",
        "    out = self.encoder_layer2_batch_norm(out)\n",
        "    out = self.encoder_layer2_pooling(out)\n",
        "\n",
        "    # Encoder Layer3\n",
        "    out = self.encoder_layer3_conv(out)\n",
        "    out = self.encoder_layer3_activation(out)\n",
        "    out = self.encoder_layer3_pooling(out)\n",
        "\n",
        "    # Encoder Layer4\n",
        "    out = self.encoder_layer4_conv(out)\n",
        "    out = self.encoder_layer4_activation(out)\n",
        "    out = self.encoder_layer4_batch_norm(out)\n",
        "    out = self.encoder_layer4_pooling(out)\n",
        "\n",
        "    # Encoder Layer5\n",
        "    out = self.encoder_layer5_conv(out)\n",
        "    out = self.encoder_layer5_activation(out)\n",
        "    out = self.encoder_layer5_pooling(out)\n",
        "\n",
        "    # Encoder Layer6\n",
        "    out = self.encoder_layer6_conv(out)\n",
        "    out = self.encoder_layer6_activation(out)\n",
        "    out = self.encoder_layer6_batch_norm(out)\n",
        "\n",
        "    # Dense Layer\n",
        "    out = self.fc1_flatten(out)\n",
        "    out = self.fc1_linear(out)\n",
        "    out = self.fc1_activation(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "uVNt1gPIR8gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7Bqh1xEXOSs",
        "outputId": "e3c4191a-f2dd-4a96-f254-0b04dc8568eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "pytorch_model_c1 = Pytorch_BUS_Final_Model_C1()\n",
        "summary(pytorch_model_c1, input_size=(1,1,180,50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwfC7_pYXRVR",
        "outputId": "a75959d4-5fc7-4764-973a-0b23e373dc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7efb8c9265f2>:111: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = self.fc1_activation(out)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Pytorch_BUS_Final_Model_C1               [1, 221]                  --\n",
              "├─Conv2d: 1-1                            [1, 8, 180, 50]           80\n",
              "├─ReLU: 1-2                              [1, 8, 180, 50]           --\n",
              "├─MaxPool2d: 1-3                         [1, 8, 90, 25]            --\n",
              "├─Conv2d: 1-4                            [1, 16, 90, 25]           1,168\n",
              "├─ReLU: 1-5                              [1, 16, 90, 25]           --\n",
              "├─BatchNorm2d: 1-6                       [1, 16, 90, 25]           32\n",
              "├─MaxPool2d: 1-7                         [1, 16, 45, 12]           --\n",
              "├─Conv2d: 1-8                            [1, 32, 45, 12]           4,640\n",
              "├─ReLU: 1-9                              [1, 32, 45, 12]           --\n",
              "├─MaxPool2d: 1-10                        [1, 32, 22, 6]            --\n",
              "├─Conv2d: 1-11                           [1, 64, 22, 6]            18,496\n",
              "├─ReLU: 1-12                             [1, 64, 22, 6]            --\n",
              "├─BatchNorm2d: 1-13                      [1, 64, 22, 6]            128\n",
              "├─MaxPool2d: 1-14                        [1, 64, 11, 3]            --\n",
              "├─Conv2d: 1-15                           [1, 128, 11, 3]           73,856\n",
              "├─ReLU: 1-16                             [1, 128, 11, 3]           --\n",
              "├─MaxPool2d: 1-17                        [1, 128, 5, 1]            --\n",
              "├─Conv2d: 1-18                           [1, 256, 5, 1]            295,168\n",
              "├─ReLU: 1-19                             [1, 256, 5, 1]            --\n",
              "├─BatchNorm2d: 1-20                      [1, 256, 5, 1]            512\n",
              "├─Flatten: 1-21                          [1, 1280]                 --\n",
              "├─Linear: 1-22                           [1, 221]                  283,101\n",
              "├─Softmax: 1-23                          [1, 221]                  --\n",
              "==========================================================================================\n",
              "Total params: 677,181\n",
              "Trainable params: 677,181\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 12.49\n",
              "==========================================================================================\n",
              "Input size (MB): 0.04\n",
              "Forward/backward pass size (MB): 1.48\n",
              "Params size (MB): 2.71\n",
              "Estimated Total Size (MB): 4.23\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch_model_c1.eval()\n",
        "# output = pytorch_model_c1(torch.Tensor(X_train[0].reshape(1,180,180,3).transpose(0,3,1,2)))\n",
        "# print(output.detach().numpy())\n",
        "# input_x = torch.tensor(X_train[0].reshape(1,180,50,1).transpose(0,3,1,2), device='cuda')\n",
        "input_x = torch.tensor(X_train[0].reshape(1,1,180,50), device='cuda').float()\n",
        "print(input_x.shape)\n",
        "output = pytorch_model_c1(input_x)\n",
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vkohHsDXE7b",
        "outputId": "b58eb412-7659-4468-c63e-7a6d012c0093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 180, 50])\n",
            "torch.Size([1, 221])\n",
            "tensor([[0.0113, 0.0036, 0.0041, 0.0080, 0.0042, 0.0036, 0.0054, 0.0054, 0.0040,\n",
            "         0.0130, 0.0025, 0.0068, 0.0049, 0.0035, 0.0035, 0.0042, 0.0037, 0.0073,\n",
            "         0.0035, 0.0039, 0.0032, 0.0037, 0.0030, 0.0016, 0.0046, 0.0048, 0.0104,\n",
            "         0.0049, 0.0023, 0.0041, 0.0030, 0.0076, 0.0044, 0.0028, 0.0027, 0.0024,\n",
            "         0.0017, 0.0113, 0.0049, 0.0049, 0.0041, 0.0062, 0.0044, 0.0032, 0.0036,\n",
            "         0.0037, 0.0070, 0.0047, 0.0033, 0.0047, 0.0023, 0.0031, 0.0024, 0.0026,\n",
            "         0.0065, 0.0050, 0.0043, 0.0067, 0.0022, 0.0059, 0.0054, 0.0039, 0.0028,\n",
            "         0.0036, 0.0032, 0.0057, 0.0049, 0.0067, 0.0107, 0.0028, 0.0039, 0.0041,\n",
            "         0.0026, 0.0052, 0.0026, 0.0030, 0.0046, 0.0028, 0.0070, 0.0042, 0.0070,\n",
            "         0.0060, 0.0023, 0.0069, 0.0015, 0.0047, 0.0052, 0.0049, 0.0025, 0.0066,\n",
            "         0.0052, 0.0047, 0.0040, 0.0056, 0.0020, 0.0050, 0.0038, 0.0049, 0.0017,\n",
            "         0.0040, 0.0061, 0.0028, 0.0024, 0.0025, 0.0086, 0.0016, 0.0028, 0.0061,\n",
            "         0.0042, 0.0032, 0.0036, 0.0061, 0.0019, 0.0058, 0.0092, 0.0033, 0.0018,\n",
            "         0.0068, 0.0062, 0.0027, 0.0045, 0.0024, 0.0025, 0.0036, 0.0061, 0.0039,\n",
            "         0.0044, 0.0041, 0.0031, 0.0051, 0.0080, 0.0026, 0.0020, 0.0043, 0.0052,\n",
            "         0.0031, 0.0034, 0.0015, 0.0063, 0.0031, 0.0085, 0.0071, 0.0035, 0.0032,\n",
            "         0.0026, 0.0034, 0.0041, 0.0059, 0.0033, 0.0025, 0.0035, 0.0044, 0.0042,\n",
            "         0.0037, 0.0042, 0.0016, 0.0025, 0.0169, 0.0028, 0.0035, 0.0067, 0.0054,\n",
            "         0.0058, 0.0047, 0.0044, 0.0043, 0.0076, 0.0035, 0.0028, 0.0054, 0.0025,\n",
            "         0.0060, 0.0022, 0.0042, 0.0053, 0.0041, 0.0068, 0.0060, 0.0062, 0.0032,\n",
            "         0.0019, 0.0043, 0.0057, 0.0224, 0.0042, 0.0082, 0.0051, 0.0047, 0.0046,\n",
            "         0.0033, 0.0041, 0.0020, 0.0033, 0.0035, 0.0016, 0.0068, 0.0028, 0.0021,\n",
            "         0.0022, 0.0032, 0.0044, 0.0091, 0.0036, 0.0064, 0.0019, 0.0055, 0.0028,\n",
            "         0.0024, 0.0045, 0.0053, 0.0020, 0.0018, 0.0060, 0.0063, 0.0035, 0.0031,\n",
            "         0.0049, 0.0033, 0.0027, 0.0048, 0.0056]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7efb8c9265f2>:111: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = self.fc1_activation(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data\n",
        "training_loader = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)), batch_size=10, pin_memory='True', pin_memory_device='cuda', shuffle=True)\n",
        "validation_loader = DataLoader(TensorDataset(torch.tensor(X_test), torch.tensor(y_test)), batch_size=1, pin_memory='True', pin_memory_device='cuda')\n",
        "#loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "# loss_fn = torch.nn.BCELoss()\n",
        "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "# Optimizers specified in the torch.optim package\n",
        "optimizer = torch.optim.Adam(pytorch_model_c1.parameters())\n",
        "\n",
        "# # import EarlyStopping\n",
        "# from pytorchtools import EarlyStopping"
      ],
      "metadata": {
        "id": "vGUNjmwkdwTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
        "    return np.eye(num_classes, dtype='uint8')[y]\n",
        "\n",
        "pytorch_model_c1 = Pytorch_BUS_Final_Model_C1().to(torch.device('cuda'))\n",
        "\n",
        "# manaul training\n",
        "def train_one_epoch():\n",
        "    # training metrics\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "\n",
        "    # validation metrics\n",
        "    valid_loss = 0\n",
        "    valid_correct = 0\n",
        "\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    pytorch_model_c1.train(True)\n",
        "    for i, data in enumerate(training_loader,0):\n",
        "        # Every data instance is an input + label pair\n",
        "        train_input, train_label = data\n",
        "        train_input = train_input.unsqueeze(dim=1).float()\n",
        "        train_label= torch.tensor(to_categorical(y=train_label, num_classes=221)).float()\n",
        "        # train_label = train_label[:,None]\n",
        "        if len(train_label.shape)==1:\n",
        "          train_label = train_label.unsqueeze(dim=0)\n",
        "\n",
        "        train_input = train_input.to(torch.device('cuda'))\n",
        "        train_label = train_label.to(torch.device('cuda'))\n",
        "\n",
        "        # print('train_input:',train_input.shape, 'train_label:',train_label.shape)\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        train_output = pytorch_model_c1(train_input)\n",
        "        # print('train_input:',train_input.shape, 'train_label:',train_label.shape, 'train_output:',train_output.shape)\n",
        "        # print('train_label:',train_label)\n",
        "        # print('train_output:',train_output)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(train_output, train_label)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        train_loss += loss.item()\n",
        "        for batch_count in range(train_output.shape[0]):\n",
        "          if(torch.argmax(train_output[batch_count,:]) == torch.argmax(train_label[batch_count,:])):\n",
        "            train_correct += 1\n",
        "\n",
        "    # print('training epoch complete')\n",
        "    # Here, we use enumerate(validation_loader) instead of\n",
        "    # iter(validation_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    pytorch_model_c1.train(False)\n",
        "    for i, data in enumerate(validation_loader,0):\n",
        "        # Every data instance is an input + label pair\n",
        "        valid_input, valid_label = data\n",
        "\n",
        "        valid_input = valid_input.unsqueeze(dim=1).float()\n",
        "        valid_label= torch.tensor(to_categorical(y=valid_label, num_classes=221)).float()\n",
        "        if len(valid_label.shape)==1:\n",
        "          valid_label = valid_label.unsqueeze(dim=0)\n",
        "\n",
        "        valid_input = valid_input.to(torch.device('cuda'))\n",
        "        valid_label = valid_label.to(torch.device('cuda'))\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        valid_output = pytorch_model_c1(valid_input)\n",
        "\n",
        "        # print('valid_input:',valid_input.shape, 'valid_label:',valid_label.shape, 'valid_output:',valid_output.shape)\n",
        "\n",
        "        # Gather data and report\n",
        "        valid_loss += loss_fn(valid_output, valid_label).item()\n",
        "        for batch_count in range(valid_output.shape[0]):\n",
        "          if(torch.argmax(valid_output[batch_count,:]) == torch.argmax(valid_label[batch_count,:])):\n",
        "            valid_correct += 1\n",
        "\n",
        "    print(f\"Training: \\n Training Accuracy: {100*train_correct/len(training_loader.dataset)}%, Average Training Loss: {train_loss/len(training_loader)}\")\n",
        "\n",
        "    print(f\"Validation: \\n Validation Accuracy: {100*valid_correct/len(validation_loader.dataset)}%, Average Validation Loss: {valid_loss/len(validation_loader)}\")\n",
        "\n",
        "    return train_loss, valid_loss\n"
      ],
      "metadata": {
        "id": "egUMDCzvdHRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "epoch_number = 0\n",
        "EPOCHS = 100\n",
        "optimizer = torch.optim.Adam(pytorch_model_c1.parameters(), lr=1e-4)\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "    train_loss, valid_loss = train_one_epoch()\n",
        "    epoch_number += 1"
      ],
      "metadata": {
        "id": "MyKfEMIfd1FY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed7554b4-a522-41a6-c566-7fc66d0cb55a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7efb8c9265f2>:111: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = self.fc1_activation(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n",
            "Training: \n",
            " Training Accuracy: 1.2244897959183674%, Average Training Loss: 5.397408135083257\n",
            "Validation: \n",
            " Validation Accuracy: 5.6105610561056105%, Average Validation Loss: 5.387497053681427\n",
            "EPOCH 2:\n",
            "Training: \n",
            " Training Accuracy: 6.938775510204081%, Average Training Loss: 5.381482163254096\n",
            "Validation: \n",
            " Validation Accuracy: 6.930693069306931%, Average Validation Loss: 5.365742341913406\n",
            "EPOCH 3:\n",
            "Training: \n",
            " Training Accuracy: 11.63265306122449%, Average Training Loss: 5.345079519310776\n",
            "Validation: \n",
            " Validation Accuracy: 11.221122112211221%, Average Validation Loss: 5.322688005151528\n",
            "EPOCH 4:\n",
            "Training: \n",
            " Training Accuracy: 15.306122448979592%, Average Training Loss: 5.291371948864995\n",
            "Validation: \n",
            " Validation Accuracy: 14.521452145214521%, Average Validation Loss: 5.278337691089894\n",
            "EPOCH 5:\n",
            "Training: \n",
            " Training Accuracy: 21.836734693877553%, Average Training Loss: 5.245112526173494\n",
            "Validation: \n",
            " Validation Accuracy: 19.141914191419144%, Average Validation Loss: 5.248409109147075\n",
            "EPOCH 6:\n",
            "Training: \n",
            " Training Accuracy: 32.857142857142854%, Average Training Loss: 5.1654886907460735\n",
            "Validation: \n",
            " Validation Accuracy: 32.34323432343234%, Average Validation Loss: 5.190383525571414\n",
            "EPOCH 7:\n",
            "Training: \n",
            " Training Accuracy: 44.69387755102041%, Average Training Loss: 5.061388794256716\n",
            "Validation: \n",
            " Validation Accuracy: 41.584158415841586%, Average Validation Loss: 5.119635111427937\n",
            "EPOCH 8:\n",
            "Training: \n",
            " Training Accuracy: 56.12244897959184%, Average Training Loss: 4.957366086998764\n",
            "Validation: \n",
            " Validation Accuracy: 54.12541254125413%, Average Validation Loss: 5.018520841504087\n",
            "EPOCH 9:\n",
            "Training: \n",
            " Training Accuracy: 66.3265306122449%, Average Training Loss: 4.851202215467181\n",
            "Validation: \n",
            " Validation Accuracy: 63.03630363036304%, Average Validation Loss: 4.990068204332106\n",
            "EPOCH 10:\n",
            "Training: \n",
            " Training Accuracy: 76.93877551020408%, Average Training Loss: 4.754766756174516\n",
            "Validation: \n",
            " Validation Accuracy: 66.33663366336634%, Average Validation Loss: 4.926215799728243\n",
            "EPOCH 11:\n",
            "Training: \n",
            " Training Accuracy: 82.24489795918367%, Average Training Loss: 4.676308096671591\n",
            "Validation: \n",
            " Validation Accuracy: 73.92739273927393%, Average Validation Loss: 4.905456187308031\n",
            "EPOCH 12:\n",
            "Training: \n",
            " Training Accuracy: 87.55102040816327%, Average Training Loss: 4.6100312933629874\n",
            "Validation: \n",
            " Validation Accuracy: 79.86798679867987%, Average Validation Loss: 4.834741872529386\n",
            "EPOCH 13:\n",
            "Training: \n",
            " Training Accuracy: 91.42857142857143%, Average Training Loss: 4.5521076552721915\n",
            "Validation: \n",
            " Validation Accuracy: 83.82838283828383%, Average Validation Loss: 4.764027184779101\n",
            "EPOCH 14:\n",
            "Training: \n",
            " Training Accuracy: 93.6734693877551%, Average Training Loss: 4.518895217350551\n",
            "Validation: \n",
            " Validation Accuracy: 85.8085808580858%, Average Validation Loss: 4.758176438485829\n",
            "EPOCH 15:\n",
            "Training: \n",
            " Training Accuracy: 96.12244897959184%, Average Training Loss: 4.485817257238894\n",
            "Validation: \n",
            " Validation Accuracy: 86.79867986798679%, Average Validation Loss: 4.790810205755454\n",
            "EPOCH 16:\n",
            "Training: \n",
            " Training Accuracy: 97.34693877551021%, Average Training Loss: 4.462695394243513\n",
            "Validation: \n",
            " Validation Accuracy: 87.12871287128714%, Average Validation Loss: 4.725235992532359\n",
            "EPOCH 17:\n",
            "Training: \n",
            " Training Accuracy: 97.55102040816327%, Average Training Loss: 4.451350202365798\n",
            "Validation: \n",
            " Validation Accuracy: 86.46864686468646%, Average Validation Loss: 4.720874326850715\n",
            "EPOCH 18:\n",
            "Training: \n",
            " Training Accuracy: 98.57142857142857%, Average Training Loss: 4.4427443815737355\n",
            "Validation: \n",
            " Validation Accuracy: 86.79867986798679%, Average Validation Loss: 4.743003571387565\n",
            "EPOCH 19:\n",
            "Training: \n",
            " Training Accuracy: 99.18367346938776%, Average Training Loss: 4.432891641344343\n",
            "Validation: \n",
            " Validation Accuracy: 89.10891089108911%, Average Validation Loss: 4.678783654379766\n",
            "EPOCH 20:\n",
            "Training: \n",
            " Training Accuracy: 99.18367346938776%, Average Training Loss: 4.429552944339052\n",
            "Validation: \n",
            " Validation Accuracy: 87.12871287128714%, Average Validation Loss: 4.682661818199032\n",
            "EPOCH 21:\n",
            "Training: \n",
            " Training Accuracy: 99.18367346938776%, Average Training Loss: 4.424075603485107\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.67089583614085\n",
            "EPOCH 22:\n",
            "Training: \n",
            " Training Accuracy: 99.18367346938776%, Average Training Loss: 4.422125213000239\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.636283798973159\n",
            "EPOCH 23:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.421011155965377\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.675697292038317\n",
            "EPOCH 24:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.4182382408453496\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.657827857303934\n",
            "EPOCH 25:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.417470046452114\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.655694944236932\n",
            "EPOCH 26:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.416670624090701\n",
            "Validation: \n",
            " Validation Accuracy: 88.77887788778878%, Average Validation Loss: 4.638040640173179\n",
            "EPOCH 27:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.416266635972626\n",
            "Validation: \n",
            " Validation Accuracy: 89.10891089108911%, Average Validation Loss: 4.660783175981478\n",
            "EPOCH 28:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.4160968235560825\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.634506150047378\n",
            "EPOCH 29:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.4156536958655535\n",
            "Validation: \n",
            " Validation Accuracy: 88.44884488448845%, Average Validation Loss: 4.643631728962309\n",
            "EPOCH 30:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.415676029361024\n",
            "Validation: \n",
            " Validation Accuracy: 88.44884488448845%, Average Validation Loss: 4.632976391921343\n",
            "EPOCH 31:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.414729624378438\n",
            "Validation: \n",
            " Validation Accuracy: 88.77887788778878%, Average Validation Loss: 4.6281582640342585\n",
            "EPOCH 32:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412616632422623\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.631587975882854\n",
            "EPOCH 33:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412727248911955\n",
            "Validation: \n",
            " Validation Accuracy: 88.77887788778878%, Average Validation Loss: 4.624715359690953\n",
            "EPOCH 34:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412588576881253\n",
            "Validation: \n",
            " Validation Accuracy: 90.42904290429043%, Average Validation Loss: 4.6175802517252\n",
            "EPOCH 35:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412109598821523\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.623340597247133\n",
            "EPOCH 36:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.412892322151029\n",
            "Validation: \n",
            " Validation Accuracy: 90.42904290429043%, Average Validation Loss: 4.627936412005535\n",
            "EPOCH 37:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411963686651113\n",
            "Validation: \n",
            " Validation Accuracy: 88.77887788778878%, Average Validation Loss: 4.626065913600104\n",
            "EPOCH 38:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411745713681591\n",
            "Validation: \n",
            " Validation Accuracy: 89.10891089108911%, Average Validation Loss: 4.605744083329003\n",
            "EPOCH 39:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4116610215634715\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.613548765088072\n",
            "EPOCH 40:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411894681502361\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.601047404135021\n",
            "EPOCH 41:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4116785964187315\n",
            "Validation: \n",
            " Validation Accuracy: 88.77887788778878%, Average Validation Loss: 4.617015243756889\n",
            "EPOCH 42:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4114623945586535\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.611033135908271\n",
            "EPOCH 43:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411559610950704\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.613288147614734\n",
            "EPOCH 44:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411292835157745\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.615663046884064\n",
            "EPOCH 45:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411194509389449\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.605283350047499\n",
            "EPOCH 46:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411178209343735\n",
            "Validation: \n",
            " Validation Accuracy: 89.10891089108911%, Average Validation Loss: 4.606348254893086\n",
            "EPOCH 47:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411022556071379\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.597986525041436\n",
            "EPOCH 48:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.411080340949857\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.605116774933566\n",
            "EPOCH 49:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410942379309207\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.595516365353424\n",
            "EPOCH 50:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410164258917984\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.608051702921147\n",
            "EPOCH 51:\n",
            "Training: \n",
            " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.409677009193265\n",
            "Validation: \n",
            " Validation Accuracy: 88.44884488448845%, Average Validation Loss: 4.622056199379093\n",
            "EPOCH 52:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4117216674648985\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.612210355576115\n",
            "EPOCH 53:\n",
            "Training: \n",
            " Training Accuracy: 99.79591836734694%, Average Training Loss: 4.408898908264783\n",
            "Validation: \n",
            " Validation Accuracy: 88.77887788778878%, Average Validation Loss: 4.6060359060961025\n",
            "EPOCH 54:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406893895596874\n",
            "Validation: \n",
            " Validation Accuracy: 90.42904290429043%, Average Validation Loss: 4.586941857542535\n",
            "EPOCH 55:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.4068686816157125\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.605751431027655\n",
            "EPOCH 56:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406702927180699\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.600887562968944\n",
            "EPOCH 57:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.40664546343745\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.588949090183371\n",
            "EPOCH 58:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406652109963553\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.593019551569872\n",
            "EPOCH 59:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406588904711665\n",
            "Validation: \n",
            " Validation Accuracy: 88.44884488448845%, Average Validation Loss: 4.5864618084218245\n",
            "EPOCH 60:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406586831929732\n",
            "Validation: \n",
            " Validation Accuracy: 90.75907590759076%, Average Validation Loss: 4.599489715626531\n",
            "EPOCH 61:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406501302913743\n",
            "Validation: \n",
            " Validation Accuracy: 89.10891089108911%, Average Validation Loss: 4.592032937720271\n",
            "EPOCH 62:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406512416138941\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.5936170552823405\n",
            "EPOCH 63:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406486267946204\n",
            "Validation: \n",
            " Validation Accuracy: 90.42904290429043%, Average Validation Loss: 4.584121903964002\n",
            "EPOCH 64:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406442252957091\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.5813998436376995\n",
            "EPOCH 65:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406484428717166\n",
            "Validation: \n",
            " Validation Accuracy: 90.75907590759076%, Average Validation Loss: 4.582252615749246\n",
            "EPOCH 66:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406433387678497\n",
            "Validation: \n",
            " Validation Accuracy: 91.08910891089108%, Average Validation Loss: 4.589981271095402\n",
            "EPOCH 67:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.40637697492327\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.597521755168147\n",
            "EPOCH 68:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406431305165193\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.578217268776973\n",
            "EPOCH 69:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406380993979318\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.577564300877033\n",
            "EPOCH 70:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406372897478999\n",
            "Validation: \n",
            " Validation Accuracy: 91.08910891089108%, Average Validation Loss: 4.590069832188068\n",
            "EPOCH 71:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.4063138767164585\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.571185016002592\n",
            "EPOCH 72:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406362806047712\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.605027364032103\n",
            "EPOCH 73:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.40628955802139\n",
            "Validation: \n",
            " Validation Accuracy: 90.75907590759076%, Average Validation Loss: 4.580965729829895\n",
            "EPOCH 74:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406274591173444\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.57850446795473\n",
            "EPOCH 75:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406279855844926\n",
            "Validation: \n",
            " Validation Accuracy: 90.42904290429043%, Average Validation Loss: 4.578751847295478\n",
            "EPOCH 76:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406261317583979\n",
            "Validation: \n",
            " Validation Accuracy: 90.75907590759076%, Average Validation Loss: 4.56816559184109\n",
            "EPOCH 77:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406251994930968\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.585343331000199\n",
            "EPOCH 78:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406257571006308\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.582321207908907\n",
            "EPOCH 79:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406231335231236\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.571727192441229\n",
            "EPOCH 80:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406204369603371\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.575390595414064\n",
            "EPOCH 81:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406204788052306\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.586918456326224\n",
            "EPOCH 82:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406217964328065\n",
            "Validation: \n",
            " Validation Accuracy: 90.42904290429043%, Average Validation Loss: 4.572937943360986\n",
            "EPOCH 83:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406190638639489\n",
            "Validation: \n",
            " Validation Accuracy: 91.08910891089108%, Average Validation Loss: 4.577781534037574\n",
            "EPOCH 84:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406203016942861\n",
            "Validation: \n",
            " Validation Accuracy: 90.42904290429043%, Average Validation Loss: 4.595573297821649\n",
            "EPOCH 85:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406179651922109\n",
            "Validation: \n",
            " Validation Accuracy: 90.75907590759076%, Average Validation Loss: 4.5721102956891455\n",
            "EPOCH 86:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406147499473727\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.571324486937066\n",
            "EPOCH 87:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406159780463394\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.576809407854237\n",
            "EPOCH 88:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406139704645897\n",
            "Validation: \n",
            " Validation Accuracy: 90.75907590759076%, Average Validation Loss: 4.574260510233763\n",
            "EPOCH 89:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406122139522007\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.567389434713735\n",
            "EPOCH 90:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406140376110466\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.589126964606861\n",
            "EPOCH 91:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406115492995904\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.570262674451268\n",
            "EPOCH 92:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406117225179867\n",
            "Validation: \n",
            " Validation Accuracy: 91.08910891089108%, Average Validation Loss: 4.56774631506539\n",
            "EPOCH 93:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406111775612344\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.567328081666046\n",
            "EPOCH 94:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406109994771529\n",
            "Validation: \n",
            " Validation Accuracy: 91.08910891089108%, Average Validation Loss: 4.56512465492727\n",
            "EPOCH 95:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.40608396335524\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.572520029426801\n",
            "EPOCH 96:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406102793557303\n",
            "Validation: \n",
            " Validation Accuracy: 90.42904290429043%, Average Validation Loss: 4.564339242752629\n",
            "EPOCH 97:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406099046979632\n",
            "Validation: \n",
            " Validation Accuracy: 90.42904290429043%, Average Validation Loss: 4.567504620001261\n",
            "EPOCH 98:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.406081647289042\n",
            "Validation: \n",
            " Validation Accuracy: 91.08910891089108%, Average Validation Loss: 4.563964255178722\n",
            "EPOCH 99:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.40608210466346\n",
            "Validation: \n",
            " Validation Accuracy: 90.75907590759076%, Average Validation Loss: 4.5716865165005425\n",
            "EPOCH 100:\n",
            "Training: \n",
            " Training Accuracy: 100.0%, Average Training Loss: 4.4060863670037715\n",
            "Validation: \n",
            " Validation Accuracy: 89.76897689768977%, Average Validation Loss: 4.573260472552611\n"
          ]
        }
      ]
    }
  ]
}