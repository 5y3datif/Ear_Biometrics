{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shujaat123/Ear_Biometrics/blob/main/Clean_Pytorch_Implementation_Ear_Biometric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57mAz8uQMn9G",
        "outputId": "6621ed84-20a5-4d0b-b062-762708201746"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "Requirement already satisfied: py7zr in /usr/local/lib/python3.10/dist-packages (0.20.5)\n",
            "Requirement already satisfied: texttable in /usr/local/lib/python3.10/dist-packages (from py7zr) (1.6.7)\n",
            "Requirement already satisfied: pycryptodomex>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from py7zr) (3.18.0)\n",
            "Requirement already satisfied: pyzstd>=0.14.4 in /usr/local/lib/python3.10/dist-packages (from py7zr) (0.15.9)\n",
            "Requirement already satisfied: pyppmd<1.1.0,>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from py7zr) (1.0.0)\n",
            "Requirement already satisfied: pybcj>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from py7zr) (1.0.1)\n",
            "Requirement already satisfied: multivolumefile>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from py7zr) (0.2.3)\n",
            "Requirement already satisfied: brotli>=1.0.9 in /usr/local/lib/python3.10/dist-packages (from py7zr) (1.0.9)\n",
            "Requirement already satisfied: inflate64>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from py7zr) (0.3.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from py7zr) (5.9.5)\n"
          ]
        }
      ],
      "source": [
        "## Load useful packages\n",
        "!pip install wget\n",
        "!pip install py7zr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import py7zr\n",
        "from zipfile import ZipFile\n",
        "from random import sample\n",
        "import PIL.Image as Image\n",
        "import matplotlib.pyplot as plt\n",
        "from  sklearn.model_selection import train_test_split\n",
        "from os import listdir\n",
        "from os import path\n",
        "import h5py\n",
        "import numpy as np\n",
        "import wget"
      ],
      "metadata": {
        "id": "WaA0QTb9MwPK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING IITD Dataset\n",
        "data_path = 'https://github.com/Shujaat123/Ear_Biometrics/blob/main/datasets/IITD_Dataset.7z?raw=true'\n",
        "filename = 'IITD_Dataset.7z'\n",
        "if(path.exists(filename)):\n",
        "  !rm $filename\n",
        "  print('existing file:', filename, ' has been deleted')\n",
        "print('downloading latest version of file:', filename)\n",
        "wget.download(data_path, filename)\n",
        "print('DONE')\n",
        "\n",
        "with py7zr.SevenZipFile('IITD_Dataset.7z', mode='r') as z:\n",
        "    z.extractall()\n",
        "!ls\n",
        "\n",
        "# Processing IITD_dataset\n",
        "src_dir = 'ear/processed/221'\n",
        "images_name = listdir(src_dir)\n",
        "images_name_temp = []\n",
        "subjects = []\n",
        "for img_ind in range(0,len(images_name)):\n",
        "  if(not(images_name[img_ind]=='Thumbs.db')):\n",
        "    subjects.append(int(images_name[img_ind].split('_')[0]))\n",
        "    images_name_temp.append(images_name[img_ind])\n",
        "\n",
        "images_name = images_name_temp\n",
        "images_name_ord = []\n",
        "subjects_ord = []\n",
        "\n",
        "sub_ind = sorted(range(len(subjects)),key=subjects.__getitem__)\n",
        "for pos, item in enumerate(sub_ind):\n",
        "  images_name_ord.append(images_name[item])\n",
        "  subjects_ord.append(subjects[item])\n",
        "\n",
        "images_name = images_name_ord\n",
        "subjects = subjects_ord\n",
        "\n",
        "print(subjects)\n",
        "print(images_name)\n",
        "\n",
        "img_ind = 0\n",
        "ear_images = []\n",
        "sub_labels = [];\n",
        "target_size = (180, 50)\n",
        "\n",
        "for sub_ind in range(0,len(subjects)):\n",
        "  img_path = src_dir+'/'+images_name[sub_ind]\n",
        "  ear_img = (plt.imread(img_path))/255\n",
        "\n",
        "  ear_img = Image.open(img_path)\n",
        "  ear_img = ear_img.resize(target_size, Image.ANTIALIAS)\n",
        "  ear_img = np.asarray(ear_img).astype(np.float32)/255\n",
        "\n",
        "  ear_images.append(ear_img)\n",
        "  sub_labels.append(subjects[sub_ind]-1)\n",
        "\n",
        "ear_images = np.array(ear_images)\n",
        "sub_labels = np.array(sub_labels)\n",
        "\n",
        "print(ear_images.shape)\n",
        "print(sub_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-gSS5XGlpPy",
        "outputId": "7f6d9569-b02e-4941-ff56-330696cc7c34"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "existing file: IITD_Dataset.7z  has been deleted\n",
            "downloading latest version of file: IITD_Dataset.7z\n",
            "DONE\n",
            "ear  IITD_Dataset.7z  IITD_Data_Tensor.mat  IITD_Labels.mat  sample_data\n",
            "[1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 7, 7, 7, 7, 8, 8, 8, 9, 9, 9, 10, 10, 10, 11, 11, 11, 12, 12, 12, 13, 13, 13, 13, 13, 13, 14, 14, 14, 15, 15, 15, 16, 16, 16, 16, 17, 17, 17, 18, 18, 18, 19, 19, 19, 19, 20, 20, 20, 21, 21, 21, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 24, 25, 25, 25, 25, 26, 26, 26, 27, 27, 27, 27, 27, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 30, 31, 31, 31, 32, 32, 32, 32, 33, 33, 33, 34, 34, 34, 35, 35, 35, 35, 36, 36, 36, 37, 37, 37, 37, 38, 38, 38, 39, 39, 39, 40, 40, 40, 41, 41, 41, 41, 42, 42, 42, 42, 43, 43, 43, 43, 43, 44, 44, 44, 45, 45, 45, 46, 46, 46, 46, 47, 47, 47, 48, 48, 48, 49, 49, 49, 49, 49, 50, 50, 50, 50, 50, 51, 51, 51, 52, 52, 52, 52, 53, 53, 53, 54, 54, 54, 54, 54, 55, 55, 55, 55, 55, 56, 56, 56, 56, 57, 57, 57, 57, 58, 58, 58, 58, 58, 59, 59, 59, 59, 59, 60, 60, 60, 61, 61, 61, 62, 62, 62, 63, 63, 63, 63, 63, 63, 64, 64, 64, 64, 64, 64, 65, 65, 65, 66, 66, 66, 66, 67, 67, 67, 68, 68, 68, 68, 69, 69, 69, 69, 69, 70, 70, 70, 70, 71, 71, 71, 72, 72, 72, 73, 73, 73, 74, 74, 74, 75, 75, 75, 76, 76, 76, 76, 76, 76, 77, 77, 77, 78, 78, 78, 79, 79, 79, 80, 80, 80, 80, 80, 81, 81, 81, 81, 82, 82, 82, 82, 83, 83, 83, 83, 84, 84, 84, 85, 85, 85, 85, 85, 85, 86, 86, 86, 86, 86, 87, 87, 87, 87, 88, 88, 88, 88, 88, 89, 89, 89, 89, 90, 90, 90, 90, 91, 91, 91, 91, 91, 91, 92, 92, 92, 93, 93, 93, 93, 93, 93, 94, 94, 94, 94, 95, 95, 95, 95, 96, 96, 96, 96, 96, 96, 97, 97, 97, 98, 98, 98, 98, 99, 99, 99, 99, 99, 99, 100, 100, 100, 101, 101, 101, 101, 101, 101, 102, 102, 102, 102, 102, 103, 103, 103, 103, 104, 104, 104, 104, 105, 105, 105, 106, 106, 106, 107, 107, 107, 107, 108, 108, 108, 108, 109, 109, 109, 110, 110, 110, 111, 111, 111, 112, 112, 112, 112, 113, 113, 113, 114, 114, 114, 114, 114, 115, 115, 115, 115, 116, 116, 116, 116, 116, 117, 117, 117, 117, 118, 118, 118, 119, 119, 119, 119, 120, 120, 120, 120, 121, 121, 121, 122, 122, 122, 123, 123, 123, 124, 124, 124, 125, 125, 125, 126, 126, 126, 127, 127, 127, 128, 128, 128, 129, 129, 129, 130, 130, 130, 131, 131, 131, 131, 132, 132, 132, 133, 133, 133, 134, 134, 134, 135, 135, 135, 136, 136, 136, 137, 137, 137, 138, 138, 138, 139, 139, 139, 140, 140, 140, 141, 141, 141, 141, 142, 142, 142, 143, 143, 143, 144, 144, 144, 145, 145, 145, 146, 146, 146, 147, 147, 147, 148, 148, 148, 149, 149, 149, 150, 150, 150, 151, 151, 151, 152, 152, 152, 153, 153, 153, 154, 154, 154, 155, 155, 155, 156, 156, 156, 157, 157, 157, 158, 158, 158, 159, 159, 159, 160, 160, 160, 161, 161, 161, 162, 162, 162, 163, 163, 163, 164, 164, 164, 165, 165, 165, 166, 166, 166, 167, 167, 167, 168, 168, 168, 169, 169, 169, 170, 170, 170, 171, 171, 171, 172, 172, 172, 173, 173, 173, 174, 174, 174, 175, 175, 175, 176, 176, 176, 177, 177, 177, 178, 178, 178, 179, 179, 179, 180, 180, 180, 180, 181, 181, 181, 181, 182, 182, 182, 183, 183, 183, 184, 184, 184, 185, 185, 185, 186, 186, 186, 187, 187, 187, 188, 188, 188, 189, 189, 189, 189, 190, 190, 190, 191, 191, 191, 191, 192, 192, 192, 193, 193, 193, 194, 194, 194, 195, 195, 195, 196, 196, 196, 196, 197, 197, 197, 198, 198, 198, 199, 199, 199, 200, 200, 200, 201, 201, 201, 202, 202, 202, 203, 203, 203, 203, 204, 204, 204, 205, 205, 205, 206, 206, 206, 207, 207, 207, 208, 208, 208, 209, 209, 209, 210, 210, 210, 211, 211, 211, 212, 212, 212, 212, 212, 213, 213, 213, 213, 213, 213, 214, 214, 214, 214, 214, 214, 215, 215, 215, 215, 216, 216, 216, 216, 217, 217, 217, 218, 218, 218, 218, 218, 218, 219, 219, 219, 219, 219, 220, 220, 220, 221, 221, 221]\n",
            "['001_3.bmp', '001_2.bmp', '001_5.bmp', '001_1.bmp', '001_6.bmp', '001_4.bmp', '002_3.bmp', '002_1.bmp', '002_2.bmp', '003_1.bmp', '003_4.bmp', '003_5.bmp', '003_3.bmp', '003_2.bmp', '004_6.bmp', '004_5.bmp', '004_4.bmp', '004_1.bmp', '004_2.bmp', '004_3.bmp', '005_2.bmp', '005_1.bmp', '005_4.bmp', '005_3.bmp', '006_3.bmp', '006_2.bmp', '006_1.bmp', '007_1.bmp', '007_2.bmp', '007_3.bmp', '007_4.bmp', '008_2.bmp', '008_3.bmp', '008_1.bmp', '009_2.bmp', '009_1.bmp', '009_3.bmp', '010_3.bmp', '010_1.bmp', '010_2.bmp', '011_1.bmp', '011_3.bmp', '011_2.bmp', '012_1.bmp', '012_3.bmp', '012_2.bmp', '013_3.bmp', '013_1.bmp', '013_5.bmp', '013_4.bmp', '013_6.bmp', '013_2.bmp', '014_3.bmp', '014_1.bmp', '014_2.bmp', '015_3.bmp', '015_2.bmp', '015_1.bmp', '016_3.bmp', '016_4.bmp', '016_1.bmp', '016_2.bmp', '017_3.bmp', '017_2.bmp', '017_1.bmp', '018_2.bmp', '018_3.bmp', '018_1.bmp', '019_1.bmp', '019_4.bmp', '019_2.bmp', '019_3.bmp', '020_1.bmp', '020_2.bmp', '020_3.bmp', '021_1.bmp', '021_3.bmp', '021_2.bmp', '022_2.bmp', '022_1.bmp', '022_3.bmp', '023_2.bmp', '023_1.bmp', '023_3.bmp', '023_4.bmp', '024_4.bmp', '024_1.bmp', '024_2.bmp', '024_3.bmp', '025_2.bmp', '025_4.bmp', '025_1.bmp', '025_3.bmp', '026_3.bmp', '026_1.bmp', '026_2.bmp', '027_4.bmp', '027_2.bmp', '027_5.bmp', '027_1.bmp', '027_3.bmp', '028_2.bmp', '028_1.bmp', '028_3.bmp', '029_2.bmp', '029_1.bmp', '029_3.bmp', '029_4.bmp', '030_4.bmp', '030_2.bmp', '030_1.bmp', '030_3.bmp', '031_3.bmp', '031_2.bmp', '031_1.bmp', '032_1.bmp', '032_4.bmp', '032_2.bmp', '032_3.bmp', '033_3.bmp', '033_1.bmp', '033_2.bmp', '034_3.bmp', '034_1.bmp', '034_2.bmp', '035_2.bmp', '035_4.bmp', '035_1.bmp', '035_3.bmp', '036_2.bmp', '036_3.bmp', '036_1.bmp', '037_3.bmp', '037_4.bmp', '037_2.bmp', '037_1.bmp', '038_1.bmp', '038_2.bmp', '038_3.bmp', '039_3.bmp', '039_2.bmp', '039_1.bmp', '040_2.bmp', '040_1.bmp', '040_3.bmp', '041_2.bmp', '041_4.bmp', '041_1.bmp', '041_3.bmp', '042_1.bmp', '042_3.bmp', '042_4.bmp', '042_2.bmp', '043_2.bmp', '043_4.bmp', '043_5.bmp', '043_3.bmp', '043_1.bmp', '044_1.bmp', '044_2.bmp', '044_3.bmp', '045_1.bmp', '045_3.bmp', '045_2.bmp', '046_2.bmp', '046_3.bmp', '046_4.bmp', '046_1.bmp', '047_1.bmp', '047_2.bmp', '047_3.bmp', '048_2.bmp', '048_3.bmp', '048_1.bmp', '049_1.bmp', '049_4.bmp', '049_3.bmp', '049_5.bmp', '049_2.bmp', '050_2.bmp', '050_4.bmp', '050_1.bmp', '050_5.bmp', '050_3.bmp', '051_2.bmp', '051_1.bmp', '051_3.bmp', '052_4.bmp', '052_1.bmp', '052_2.bmp', '052_3.bmp', '053_1.bmp', '053_2.bmp', '053_3.bmp', '054_3.bmp', '054_2.bmp', '054_4.bmp', '054_5.bmp', '054_1.bmp', '055_1.bmp', '055_5.bmp', '055_3.bmp', '055_4.bmp', '055_2.bmp', '056_4.bmp', '056_2.bmp', '056_1.bmp', '056_3.bmp', '057_1.bmp', '057_3.bmp', '057_4.bmp', '057_2.bmp', '058_4.bmp', '058_3.bmp', '058_2.bmp', '058_5.bmp', '058_1.bmp', '059_3.bmp', '059_4.bmp', '059_1.bmp', '059_5.bmp', '059_2.bmp', '060_3.bmp', '060_1.bmp', '060_2.bmp', '061_2.bmp', '061_1.bmp', '061_3.bmp', '062_3.bmp', '062_2.bmp', '062_1.bmp', '063_1.bmp', '063_3.bmp', '063_2.bmp', '063_4.bmp', '063_5.bmp', '063_6.bmp', '064_1.bmp', '064_5.bmp', '064_6.bmp', '064_3.bmp', '064_4.bmp', '064_2.bmp', '065_1.bmp', '065_3.bmp', '065_2.bmp', '066_4.bmp', '066_1.bmp', '066_2.bmp', '066_3.bmp', '067_2.bmp', '067_1.bmp', '067_3.bmp', '068_4.bmp', '068_2.bmp', '068_1.bmp', '068_3.bmp', '069_5.bmp', '069_2.bmp', '069_3.bmp', '069_1.bmp', '069_4.bmp', '070_2.bmp', '070_3.bmp', '070_4.bmp', '070_1.bmp', '071_2.bmp', '071_1.bmp', '071_3.bmp', '072_1.bmp', '072_3.bmp', '072_2.bmp', '073_3.bmp', '073_2.bmp', '073_1.bmp', '074_1.bmp', '074_2.bmp', '074_3.bmp', '075_1.bmp', '075_2.bmp', '075_3.bmp', '076_5.bmp', '076_3.bmp', '076_1.bmp', '076_4.bmp', '076_6.bmp', '076_2.bmp', '077_2.bmp', '077_1.bmp', '077_3.bmp', '078_1.bmp', '078_3.bmp', '078_2.bmp', '079_3.bmp', '079_2.bmp', '079_1.bmp', '080_5.bmp', '080_2.bmp', '080_4.bmp', '080_1.bmp', '080_3.bmp', '081_4.bmp', '081_1.bmp', '081_3.bmp', '081_2.bmp', '082_4.bmp', '082_3.bmp', '082_2.bmp', '082_1.bmp', '083_2.bmp', '083_1.bmp', '083_3.bmp', '083_4.bmp', '084_2.bmp', '084_3.bmp', '084_1.bmp', '085_3.bmp', '085_4.bmp', '085_5.bmp', '085_2.bmp', '085_6.bmp', '085_1.bmp', '086_1.bmp', '086_5.bmp', '086_3.bmp', '086_2.bmp', '086_4.bmp', '087_3.bmp', '087_2.bmp', '087_4.bmp', '087_1.bmp', '088_3.bmp', '088_2.bmp', '088_1.bmp', '088_5.bmp', '088_4.bmp', '089_2.bmp', '089_1.bmp', '089_4.bmp', '089_3.bmp', '090_1.bmp', '090_3.bmp', '090_2.bmp', '090_4.bmp', '091_6.bmp', '091_2.bmp', '091_5.bmp', '091_4.bmp', '091_1.bmp', '091_3.bmp', '092_2.bmp', '092_1.bmp', '092_3.bmp', '093_1.bmp', '093_6.bmp', '093_2.bmp', '093_5.bmp', '093_3.bmp', '093_4.bmp', '094_3.bmp', '094_4.bmp', '094_1.bmp', '094_2.bmp', '095_1.bmp', '095_4.bmp', '095_2.bmp', '095_3.bmp', '096_3.bmp', '096_4.bmp', '096_5.bmp', '096_1.bmp', '096_6.bmp', '096_2.bmp', '097_3.bmp', '097_2.bmp', '097_1.bmp', '098_3.bmp', '098_1.bmp', '098_4.bmp', '098_2.bmp', '099_2.bmp', '099_1.bmp', '099_6.bmp', '099_5.bmp', '099_3.bmp', '099_4.bmp', '100_1.bmp', '100_3.bmp', '100_2.bmp', '101_6.bmp', '101_2.bmp', '101_5.bmp', '101_3.bmp', '101_4.bmp', '101_1.bmp', '102_1.bmp', '102_5.bmp', '102_3.bmp', '102_2.bmp', '102_4.bmp', '103_4.bmp', '103_3.bmp', '103_2.bmp', '103_1.bmp', '104_2.bmp', '104_4.bmp', '104_1.bmp', '104_3.bmp', '105_2.bmp', '105_3.bmp', '105_1.bmp', '106_2.bmp', '106_3.bmp', '106_1.bmp', '107_4.bmp', '107_1.bmp', '107_2.bmp', '107_3.bmp', '108_1.bmp', '108_2.bmp', '108_3.bmp', '108_4.bmp', '109_3.bmp', '109_1.bmp', '109_2.bmp', '110_3.bmp', '110_1.bmp', '110_2.bmp', '111_1.bmp', '111_2.bmp', '111_3.bmp', '112_4.bmp', '112_2.bmp', '112_1.bmp', '112_3.bmp', '113_2.bmp', '113_1.bmp', '113_3.bmp', '114_4.bmp', '114_3.bmp', '114_1.bmp', '114_2.bmp', '114_5.bmp', '115_4.bmp', '115_2.bmp', '115_1.bmp', '115_3.bmp', '116_2.bmp', '116_1.bmp', '116_5.bmp', '116_4.bmp', '116_3.bmp', '117_2.bmp', '117_1.bmp', '117_3.bmp', '117_4.bmp', '118_1.bmp', '118_2.bmp', '118_3.bmp', '119_3.bmp', '119_2.bmp', '119_1.bmp', '119_4.bmp', '120_4.bmp', '120_3.bmp', '120_2.bmp', '120_1.bmp', '121_3.bmp', '121_2.bmp', '121_1.bmp', '122_2.bmp', '122_1.bmp', '122_3.bmp', '123_1.bmp', '123_3.bmp', '123_2.bmp', '124_3.bmp', '124_2.bmp', '124_1.bmp', '125_3.bmp', '125_2.bmp', '125_1.bmp', '126_2.bmp', '126_3.bmp', '126_1.bmp', '127_1.bmp', '127_2.bmp', '127_3.bmp', '128_3.bmp', '128_2.bmp', '128_1.bmp', '129_1.bmp', '129_3.bmp', '129_2.bmp', '130_1.bmp', '130_3.bmp', '130_2.bmp', '131_2.bmp', '131_3.bmp', '131_1.bmp', '131_4.bmp', '132_1.bmp', '132_2.bmp', '132_3.bmp', '133_3.bmp', '133_2.bmp', '133_1.bmp', '134_3.bmp', '134_1.bmp', '134_2.bmp', '135_2.bmp', '135_1.bmp', '135_3.bmp', '136_2.bmp', '136_1.bmp', '136_3.bmp', '137_2.bmp', '137_3.bmp', '137_1.bmp', '138_3.bmp', '138_1.bmp', '138_2.bmp', '139_3.bmp', '139_2.bmp', '139_1.bmp', '140_3.bmp', '140_2.bmp', '140_1.bmp', '141_3.bmp', '141_1.bmp', '141_4.bmp', '141_2.bmp', '142_3.bmp', '142_2.bmp', '142_1.bmp', '143_3.bmp', '143_2.bmp', '143_1.bmp', '144_3.bmp', '144_1.bmp', '144_2.bmp', '145_2.bmp', '145_3.bmp', '145_1.bmp', '146_3.bmp', '146_2.bmp', '146_1.bmp', '147_1.bmp', '147_2.bmp', '147_3.bmp', '148_2.bmp', '148_3.bmp', '148_1.bmp', '149_3.bmp', '149_2.bmp', '149_1.bmp', '150_1.bmp', '150_3.bmp', '150_2.bmp', '151_1.bmp', '151_3.bmp', '151_2.bmp', '152_1.bmp', '152_3.bmp', '152_2.bmp', '153_3.bmp', '153_1.bmp', '153_2.bmp', '154_3.bmp', '154_2.bmp', '154_1.bmp', '155_1.bmp', '155_2.bmp', '155_3.bmp', '156_2.bmp', '156_3.bmp', '156_1.bmp', '157_3.bmp', '157_1.bmp', '157_2.bmp', '158_1.bmp', '158_2.bmp', '158_3.bmp', '159_1.bmp', '159_2.bmp', '159_3.bmp', '160_1.bmp', '160_2.bmp', '160_3.bmp', '161_3.bmp', '161_1.bmp', '161_2.bmp', '162_1.bmp', '162_3.bmp', '162_2.bmp', '163_1.bmp', '163_2.bmp', '163_3.bmp', '164_3.bmp', '164_2.bmp', '164_1.bmp', '165_1.bmp', '165_3.bmp', '165_2.bmp', '166_1.bmp', '166_2.bmp', '166_3.bmp', '167_3.bmp', '167_1.bmp', '167_2.bmp', '168_2.bmp', '168_1.bmp', '168_3.bmp', '169_3.bmp', '169_2.bmp', '169_1.bmp', '170_3.bmp', '170_2.bmp', '170_1.bmp', '171_1.bmp', '171_3.bmp', '171_2.bmp', '172_2.bmp', '172_1.bmp', '172_3.bmp', '173_2.bmp', '173_1.bmp', '173_3.bmp', '174_3.bmp', '174_2.bmp', '174_1.bmp', '175_1.bmp', '175_2.bmp', '175_3.bmp', '176_3.bmp', '176_2.bmp', '176_1.bmp', '177_1.bmp', '177_3.bmp', '177_2.bmp', '178_3.bmp', '178_2.bmp', '178_1.bmp', '179_3.bmp', '179_1.bmp', '179_2.bmp', '180_1.bmp', '180_3.bmp', '180_4.bmp', '180_2.bmp', '181_3.bmp', '181_2.bmp', '181_1.bmp', '181_4.bmp', '182_1.bmp', '182_2.bmp', '182_3.bmp', '183_2.bmp', '183_1.bmp', '183_3.bmp', '184_1.bmp', '184_3.bmp', '184_2.bmp', '185_1.bmp', '185_2.bmp', '185_3.bmp', '186_2.bmp', '186_1.bmp', '186_3.bmp', '187_1.bmp', '187_2.bmp', '187_3.bmp', '188_2.bmp', '188_3.bmp', '188_1.bmp', '189_4.bmp', '189_3.bmp', '189_1.bmp', '189_2.bmp', '190_3.bmp', '190_2.bmp', '190_1.bmp', '191_3.bmp', '191_1.bmp', '191_2.bmp', '191_4.bmp', '192_2.bmp', '192_3.bmp', '192_1.bmp', '193_2.bmp', '193_3.bmp', '193_1.bmp', '194_2.bmp', '194_1.bmp', '194_3.bmp', '195_2.bmp', '195_3.bmp', '195_1.bmp', '196_4.bmp', '196_2.bmp', '196_3.bmp', '196_1.bmp', '197_3.bmp', '197_1.bmp', '197_2.bmp', '198_2.bmp', '198_1.bmp', '198_3.bmp', '199_1.bmp', '199_3.bmp', '199_2.bmp', '200_1.bmp', '200_2.bmp', '200_3.bmp', '201_1.bmp', '201_3.bmp', '201_2.bmp', '202_3.bmp', '202_2.bmp', '202_1.bmp', '203_3.bmp', '203_2.bmp', '203_4.bmp', '203_1.bmp', '204_3.bmp', '204_2.bmp', '204_1.bmp', '205_2.bmp', '205_1.bmp', '205_3.bmp', '206_2.bmp', '206_3.bmp', '206_1.bmp', '207_3.bmp', '207_1.bmp', '207_2.bmp', '208_3.bmp', '208_1.bmp', '208_2.bmp', '209_3.bmp', '209_1.bmp', '209_2.bmp', '210_1.bmp', '210_3.bmp', '210_2.bmp', '211_1.bmp', '211_3.bmp', '211_2.bmp', '212_5.bmp', '212_4.bmp', '212_1.bmp', '212_3.bmp', '212_2.bmp', '213_3.bmp', '213_5.bmp', '213_1.bmp', '213_6.bmp', '213_2.bmp', '213_4.bmp', '214_6.bmp', '214_5.bmp', '214_3.bmp', '214_1.bmp', '214_2.bmp', '214_4.bmp', '215_1.bmp', '215_3.bmp', '215_2.bmp', '215_4.bmp', '216_1.bmp', '216_2.bmp', '216_3.bmp', '216_4.bmp', '217_3.bmp', '217_1.bmp', '217_2.bmp', '218_3.bmp', '218_6.bmp', '218_4.bmp', '218_1.bmp', '218_2.bmp', '218_5.bmp', '219_4.bmp', '219_2.bmp', '219_5.bmp', '219_1.bmp', '219_3.bmp', '220_3.bmp', '220_2.bmp', '220_1.bmp', '221_1.bmp', '221_3.bmp', '221_2.bmp']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-887dcca04adf>:50: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  ear_img = ear_img.resize(target_size, Image.ANTIALIAS)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(793, 50, 180)\n",
            "(793,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(ear_images, sub_labels, test_size=0.382093316519, random_state=42, stratify=sub_labels)\n",
        "# X_train, X_test, y_train, y_test = train_test_split(ear_images, sub_labels, test_size=0.2786885245901639, random_state=42, stratify=sub_labels)\n",
        "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.386363636363, random_state=42, stratify=y_train)\n",
        "\n",
        "print('Training dataset:\\n',X_train.shape)\n",
        "print(y_train.shape)\n",
        "# print('Validation dataset:\\n',X_valid.shape)\n",
        "# print(y_valid.shape)\n",
        "print('Test dataset:\\n',X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxumLS3QOES3",
        "outputId": "f6768640-db04-47dc-f945-0918ecb9cbee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset:\n",
            " (490, 50, 180)\n",
            "(490,)\n",
            "Test dataset:\n",
            " (303, 50, 180)\n",
            "(303,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conventional convolution"
      ],
      "metadata": {
        "id": "-Iqa_Mc5xUiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn\n",
        "import torch.nn.functional\n",
        "import torch.optim\n",
        "from torchvision import models #just for debugging"
      ],
      "metadata": {
        "id": "VbDuNIPfRIH6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pytorch_BUS_Final_Model_C1(torch.nn.Module):\n",
        "  #  Determine what layers and their order in CNN object\n",
        "  def __init__(self, num_classes=221, num_filters=8, input_shape=(180,50,1)):\n",
        "    super(Pytorch_BUS_Final_Model_C1,self).__init__()\n",
        "    #self.encoder_input = input_shape[-1]\n",
        "    kernel_size = 3\n",
        "    # Encoder Layer1\n",
        "    self.encoder_layer1_name = 'encoder_layer1'\n",
        "    self.encoder_layer1_conv = torch.nn.Conv2d(1,\n",
        "                                               num_filters,\n",
        "                                               kernel_size,\n",
        "                                               padding='same')\n",
        "\n",
        "    self.encoder_layer1_activation = torch.nn.ReLU()\n",
        "    self.encoder_layer1_pooling = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "    # Encoder Layer2\n",
        "    self.encoder_layer2_name = 'encoder_layer2'\n",
        "    self.encoder_layer2_conv = torch.nn.Conv2d(num_filters,\n",
        "                                               2*num_filters,\n",
        "                                               kernel_size,\n",
        "                                               padding='same')\n",
        "    self.encoder_layer2_activation = torch.nn.ReLU()\n",
        "    self.encoder_layer2_batch_norm = torch.nn.BatchNorm2d(2*num_filters,\n",
        "                                                          eps = 1e-3,\n",
        "                                                          momentum = 0.99)\n",
        "    self.encoder_layer2_pooling = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "    # Encoder Layer3\n",
        "    self.encoder_layer3_name = 'encoder_layer3'\n",
        "    self.encoder_layer3_conv = torch.nn.Conv2d(2*num_filters,\n",
        "                                               4*num_filters,\n",
        "                                               kernel_size,\n",
        "                                               padding='same')\n",
        "    self.encoder_layer3_activation = torch.nn.ReLU()\n",
        "    self.encoder_layer3_pooling = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "    # Encoder Layer4\n",
        "    self.encoder_layer4_name = 'encoder_layer4'\n",
        "    self.encoder_layer4_conv = torch.nn.Conv2d(4*num_filters,\n",
        "                                               8*num_filters,\n",
        "                                               kernel_size,\n",
        "                                               padding='same')\n",
        "    self.encoder_layer4_activation = torch.nn.ReLU()\n",
        "    self.encoder_layer4_batch_norm = torch.nn.BatchNorm2d(8*num_filters,\n",
        "                                                          eps = 1e-3,\n",
        "                                                          momentum = 0.99)\n",
        "    self.encoder_layer4_pooling = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "    # Encoder Layer5\n",
        "    self.encoder_layer5_name = 'encoder_layer5'\n",
        "    self.encoder_layer5_conv = torch.nn.Conv2d(8*num_filters,\n",
        "                                               16*num_filters,\n",
        "                                               kernel_size,\n",
        "                                               padding='same')\n",
        "\n",
        "    self.encoder_layer5_activation = torch.nn.ReLU()\n",
        "    self.encoder_layer5_pooling = torch.nn.MaxPool2d(kernel_size=(2, 2))\n",
        "\n",
        "   # Encoder Layer6\n",
        "    self.encoder_layer6_name = 'encoder_layer2'\n",
        "    self.encoder_layer6_conv = torch.nn.Conv2d(16*num_filters,\n",
        "                                               32*num_filters,\n",
        "                                               kernel_size,\n",
        "                                               padding='same')\n",
        "    self.encoder_layer6_activation = torch.nn.ReLU()\n",
        "    self.encoder_layer6_batch_norm = torch.nn.BatchNorm2d(32*num_filters,\n",
        "                                                          eps = 1e-3,\n",
        "                                                          momentum = 0.99)\n",
        "    # Dense layer\n",
        "    self.fc1_flatten = torch.nn.Flatten()\n",
        "    self.fc1_linear = torch.nn.Linear(32*num_filters*(input_shape[0]//(2**5))*(input_shape[1]//(2**5)), num_classes)\n",
        "    self.fc1_activation = torch.nn.Softmax()\n",
        "\n",
        "  def forward(self,x):\n",
        "    # Encoder Layer1\n",
        "    out = self.encoder_layer1_conv(x)\n",
        "    out = self.encoder_layer1_activation(out)\n",
        "    out = self.encoder_layer1_pooling(out)\n",
        "\n",
        "    # Encoder Layer2\n",
        "    out = self.encoder_layer2_conv(out)\n",
        "    out = self.encoder_layer2_activation(out)\n",
        "    out = self.encoder_layer2_batch_norm(out)\n",
        "    out = self.encoder_layer2_pooling(out)\n",
        "\n",
        "    # Encoder Layer3\n",
        "    out = self.encoder_layer3_conv(out)\n",
        "    out = self.encoder_layer3_activation(out)\n",
        "    out = self.encoder_layer3_pooling(out)\n",
        "\n",
        "    # Encoder Layer4\n",
        "    out = self.encoder_layer4_conv(out)\n",
        "    out = self.encoder_layer4_activation(out)\n",
        "    out = self.encoder_layer4_batch_norm(out)\n",
        "    out = self.encoder_layer4_pooling(out)\n",
        "\n",
        "    # Encoder Layer5\n",
        "    out = self.encoder_layer5_conv(out)\n",
        "    out = self.encoder_layer5_activation(out)\n",
        "    out = self.encoder_layer5_pooling(out)\n",
        "\n",
        "    # Encoder Layer6\n",
        "    out = self.encoder_layer6_conv(out)\n",
        "    out = self.encoder_layer6_activation(out)\n",
        "    out = self.encoder_layer6_batch_norm(out)\n",
        "\n",
        "    # Dense Layer\n",
        "    out = self.fc1_flatten(out)\n",
        "    out = self.fc1_linear(out)\n",
        "    out = self.fc1_activation(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "uVNt1gPIR8gi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7Bqh1xEXOSs",
        "outputId": "e54d8025-87dd-4aff-d8fb-53752722c2a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.10/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "pytorch_model_c1 = Pytorch_BUS_Final_Model_C1()\n",
        "summary(pytorch_model_c1, input_size=(1,1,180,50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fwfC7_pYXRVR",
        "outputId": "fe4d3f31-d75b-4e9f-fcbc-796b67ad871a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-7efb8c9265f2>:111: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = self.fc1_activation(out)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Pytorch_BUS_Final_Model_C1               [1, 221]                  --\n",
              "├─Conv2d: 1-1                            [1, 8, 180, 50]           80\n",
              "├─ReLU: 1-2                              [1, 8, 180, 50]           --\n",
              "├─MaxPool2d: 1-3                         [1, 8, 90, 25]            --\n",
              "├─Conv2d: 1-4                            [1, 16, 90, 25]           1,168\n",
              "├─ReLU: 1-5                              [1, 16, 90, 25]           --\n",
              "├─BatchNorm2d: 1-6                       [1, 16, 90, 25]           32\n",
              "├─MaxPool2d: 1-7                         [1, 16, 45, 12]           --\n",
              "├─Conv2d: 1-8                            [1, 32, 45, 12]           4,640\n",
              "├─ReLU: 1-9                              [1, 32, 45, 12]           --\n",
              "├─MaxPool2d: 1-10                        [1, 32, 22, 6]            --\n",
              "├─Conv2d: 1-11                           [1, 64, 22, 6]            18,496\n",
              "├─ReLU: 1-12                             [1, 64, 22, 6]            --\n",
              "├─BatchNorm2d: 1-13                      [1, 64, 22, 6]            128\n",
              "├─MaxPool2d: 1-14                        [1, 64, 11, 3]            --\n",
              "├─Conv2d: 1-15                           [1, 128, 11, 3]           73,856\n",
              "├─ReLU: 1-16                             [1, 128, 11, 3]           --\n",
              "├─MaxPool2d: 1-17                        [1, 128, 5, 1]            --\n",
              "├─Conv2d: 1-18                           [1, 256, 5, 1]            295,168\n",
              "├─ReLU: 1-19                             [1, 256, 5, 1]            --\n",
              "├─BatchNorm2d: 1-20                      [1, 256, 5, 1]            512\n",
              "├─Flatten: 1-21                          [1, 1280]                 --\n",
              "├─Linear: 1-22                           [1, 221]                  283,101\n",
              "├─Softmax: 1-23                          [1, 221]                  --\n",
              "==========================================================================================\n",
              "Total params: 677,181\n",
              "Trainable params: 677,181\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 12.49\n",
              "==========================================================================================\n",
              "Input size (MB): 0.04\n",
              "Forward/backward pass size (MB): 1.48\n",
              "Params size (MB): 2.71\n",
              "Estimated Total Size (MB): 4.23\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch_model_c1.eval()\n",
        "# output = pytorch_model_c1(torch.Tensor(X_train[0].reshape(1,180,180,3).transpose(0,3,1,2)))\n",
        "# print(output.detach().numpy())\n",
        "# input_x = torch.tensor(X_train[0].reshape(1,180,50,1).transpose(0,3,1,2), device='cuda')\n",
        "input_x = torch.tensor(X_train[0].reshape(1,1,180,50), device='cuda').float()\n",
        "print(input_x.shape)\n",
        "output = pytorch_model_c1(input_x)\n",
        "print(output.shape)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vkohHsDXE7b",
        "outputId": "bf2ba5bd-e42e-47ed-c588-4c2955653b5b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1, 180, 50])\n",
            "torch.Size([1, 221])\n",
            "tensor([[0.0040, 0.0055, 0.0037, 0.0024, 0.0040, 0.0040, 0.0038, 0.0040, 0.0034,\n",
            "         0.0056, 0.0039, 0.0040, 0.0024, 0.0082, 0.0123, 0.0025, 0.0066, 0.0027,\n",
            "         0.0053, 0.0076, 0.0103, 0.0056, 0.0044, 0.0092, 0.0030, 0.0027, 0.0043,\n",
            "         0.0038, 0.0043, 0.0058, 0.0074, 0.0065, 0.0058, 0.0043, 0.0039, 0.0048,\n",
            "         0.0022, 0.0054, 0.0041, 0.0035, 0.0029, 0.0040, 0.0033, 0.0025, 0.0045,\n",
            "         0.0037, 0.0066, 0.0030, 0.0092, 0.0051, 0.0045, 0.0053, 0.0141, 0.0032,\n",
            "         0.0049, 0.0045, 0.0041, 0.0067, 0.0037, 0.0045, 0.0052, 0.0075, 0.0085,\n",
            "         0.0034, 0.0033, 0.0064, 0.0040, 0.0020, 0.0089, 0.0040, 0.0051, 0.0059,\n",
            "         0.0044, 0.0030, 0.0033, 0.0098, 0.0040, 0.0048, 0.0034, 0.0042, 0.0026,\n",
            "         0.0038, 0.0030, 0.0042, 0.0032, 0.0033, 0.0089, 0.0061, 0.0041, 0.0030,\n",
            "         0.0035, 0.0043, 0.0054, 0.0088, 0.0031, 0.0054, 0.0015, 0.0048, 0.0030,\n",
            "         0.0028, 0.0034, 0.0077, 0.0060, 0.0052, 0.0022, 0.0015, 0.0017, 0.0049,\n",
            "         0.0031, 0.0059, 0.0032, 0.0053, 0.0036, 0.0087, 0.0039, 0.0043, 0.0058,\n",
            "         0.0057, 0.0039, 0.0052, 0.0035, 0.0038, 0.0045, 0.0046, 0.0031, 0.0033,\n",
            "         0.0020, 0.0043, 0.0038, 0.0037, 0.0052, 0.0038, 0.0049, 0.0025, 0.0028,\n",
            "         0.0026, 0.0054, 0.0051, 0.0033, 0.0062, 0.0044, 0.0038, 0.0030, 0.0036,\n",
            "         0.0048, 0.0052, 0.0045, 0.0033, 0.0020, 0.0046, 0.0024, 0.0037, 0.0035,\n",
            "         0.0044, 0.0045, 0.0048, 0.0037, 0.0034, 0.0096, 0.0031, 0.0036, 0.0025,\n",
            "         0.0058, 0.0031, 0.0056, 0.0065, 0.0024, 0.0025, 0.0041, 0.0057, 0.0040,\n",
            "         0.0078, 0.0033, 0.0057, 0.0050, 0.0049, 0.0027, 0.0041, 0.0047, 0.0035,\n",
            "         0.0034, 0.0036, 0.0029, 0.0035, 0.0041, 0.0012, 0.0040, 0.0033, 0.0030,\n",
            "         0.0024, 0.0067, 0.0139, 0.0068, 0.0074, 0.0044, 0.0059, 0.0043, 0.0043,\n",
            "         0.0044, 0.0042, 0.0039, 0.0019, 0.0050, 0.0033, 0.0048, 0.0045, 0.0035,\n",
            "         0.0030, 0.0041, 0.0017, 0.0041, 0.0030, 0.0056, 0.0048, 0.0058, 0.0043,\n",
            "         0.0053, 0.0033, 0.0046, 0.0042, 0.0029]], device='cuda:0',\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-7efb8c9265f2>:111: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = self.fc1_activation(out)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data\n",
        "training_loader = DataLoader(TensorDataset(torch.tensor(X_train), torch.tensor(y_train)), batch_size=10, pin_memory='True', pin_memory_device='cuda', shuffle=True)\n",
        "validation_loader = DataLoader(TensorDataset(torch.tensor(X_test), torch.tensor(y_test)), batch_size=1, pin_memory='True', pin_memory_device='cuda')\n",
        "#loss function\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "# loss_fn = torch.nn.BCELoss()\n",
        "# loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "# Optimizers specified in the torch.optim package\n",
        "optimizer = torch.optim.Adam(pytorch_model_c1.parameters())\n",
        "\n",
        "# # import EarlyStopping\n",
        "# from pytorchtools import EarlyStopping"
      ],
      "metadata": {
        "id": "vGUNjmwkdwTG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
        "    return np.eye(num_classes, dtype='uint8')[y]\n",
        "\n",
        "pytorch_model_c1 = Pytorch_BUS_Final_Model_C1().to(torch.device('cuda'))\n",
        "\n",
        "# manaul training\n",
        "def train_one_epoch():\n",
        "    # training metrics\n",
        "    train_loss = 0\n",
        "    train_correct = 0\n",
        "\n",
        "    # validation metrics\n",
        "    valid_loss = 0\n",
        "    valid_correct = 0\n",
        "\n",
        "\n",
        "    # Here, we use enumerate(training_loader) instead of\n",
        "    # iter(training_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    pytorch_model_c1.train(True)\n",
        "    for i, data in enumerate(training_loader,0):\n",
        "        # Every data instance is an input + label pair\n",
        "        train_input, train_label = data\n",
        "        train_input = train_input.unsqueeze(dim=1).float()\n",
        "        train_label= torch.tensor(to_categorical(y=train_label, num_classes=221)).float()\n",
        "        # train_label = train_label[:,None]\n",
        "        if len(train_label.shape)==1:\n",
        "          train_label = train_label.unsqueeze(dim=0)\n",
        "\n",
        "        train_input = train_input.to(torch.device('cuda'))\n",
        "        train_label = train_label.to(torch.device('cuda'))\n",
        "\n",
        "        # print('train_input:',train_input.shape, 'train_label:',train_label.shape)\n",
        "\n",
        "        # Zero your gradients for every batch!\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        train_output = pytorch_model_c1(train_input)\n",
        "        # print('train_input:',train_input.shape, 'train_label:',train_label.shape, 'train_output:',train_output.shape)\n",
        "        # print('train_label:',train_label)\n",
        "        # print('train_output:',train_output)\n",
        "\n",
        "        # Compute the loss and its gradients\n",
        "        loss = loss_fn(train_output, train_label)\n",
        "        loss.backward()\n",
        "\n",
        "        # Adjust learning weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        train_loss += loss.item()\n",
        "        for batch_count in range(train_output.shape[0]):\n",
        "          if(torch.argmax(train_output[batch_count,:]) == torch.argmax(train_label[batch_count,:])):\n",
        "            train_correct += 1\n",
        "\n",
        "    # print('training epoch complete')\n",
        "    # Here, we use enumerate(validation_loader) instead of\n",
        "    # iter(validation_loader) so that we can track the batch\n",
        "    # index and do some intra-epoch reporting\n",
        "    pytorch_model_c1.train(False)\n",
        "    for i, data in enumerate(validation_loader,0):\n",
        "        # Every data instance is an input + label pair\n",
        "        valid_input, valid_label = data\n",
        "\n",
        "        valid_input = valid_input.unsqueeze(dim=1).float()\n",
        "        valid_label= torch.tensor(to_categorical(y=valid_label, num_classes=221)).float()\n",
        "        if len(valid_label.shape)==1:\n",
        "          valid_label = valid_label.unsqueeze(dim=0)\n",
        "\n",
        "        valid_input = valid_input.to(torch.device('cuda'))\n",
        "        valid_label = valid_label.to(torch.device('cuda'))\n",
        "\n",
        "        # Make predictions for this batch\n",
        "        valid_output = pytorch_model_c1(valid_input)\n",
        "\n",
        "        # print('valid_input:',valid_input.shape, 'valid_label:',valid_label.shape, 'valid_output:',valid_output.shape)\n",
        "\n",
        "        # Gather data and report\n",
        "        valid_loss += loss_fn(valid_output, valid_label).item()\n",
        "        for batch_count in range(valid_output.shape[0]):\n",
        "          if(torch.argmax(valid_output[batch_count,:]) == torch.argmax(valid_label[batch_count,:])):\n",
        "            valid_correct += 1\n",
        "\n",
        "    print(f\"Training: \\n Training Accuracy: {100*train_correct/len(training_loader.dataset)}%, Average Training Loss: {train_loss/len(training_loader)}\")\n",
        "\n",
        "    print(f\"Validation: \\n Validation Accuracy: {100*valid_correct/len(validation_loader.dataset)}%, Average Validation Loss: {valid_loss/len(validation_loader)}\")\n",
        "\n",
        "    return train_loss, valid_loss\n"
      ],
      "metadata": {
        "id": "egUMDCzvdHRC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
        "epoch_number = 0\n",
        "EPOCHS = 100\n",
        "optimizer = torch.optim.Adam(pytorch_model_c1.parameters(), lr=1e-4)\n",
        "for epoch in range(EPOCHS):\n",
        "    print('EPOCH {}:'.format(epoch_number + 1))\n",
        "    train_loss, valid_loss = train_one_epoch()\n",
        "    epoch_number += 1"
      ],
      "metadata": {
        "id": "MyKfEMIfd1FY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eeb48b7-ddc7-4de0-92f8-c29a07d32163"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH 1:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-7efb8c9265f2>:111: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  out = self.fc1_activation(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training: \n",
            " Training Accuracy: 1.2244897959183674%, Average Training Loss: 5.397212077160271\n",
            "Validation: \n",
            " Validation Accuracy: 3.3003300330033003%, Average Validation Loss: 5.390010459194876\n",
            "EPOCH 2:\n",
            "Training: \n",
            " Training Accuracy: 5.510204081632653%, Average Training Loss: 5.380439339851846\n",
            "Validation: \n",
            " Validation Accuracy: 5.6105610561056105%, Average Validation Loss: 5.366038734763369\n",
            "EPOCH 3:\n",
            "Training: \n",
            " Training Accuracy: 9.183673469387756%, Average Training Loss: 5.341619666741819\n",
            "Validation: \n",
            " Validation Accuracy: 10.231023102310232%, Average Validation Loss: 5.32459035722336\n",
            "EPOCH 4:\n",
            "Training: \n",
            " Training Accuracy: 16.53061224489796%, Average Training Loss: 5.29768399803006\n",
            "Validation: \n",
            " Validation Accuracy: 15.841584158415841%, Average Validation Loss: 5.285597738259696\n",
            "EPOCH 5:\n",
            "Training: \n",
            " Training Accuracy: 27.551020408163264%, Average Training Loss: 5.234065513221585\n",
            "Validation: \n",
            " Validation Accuracy: 27.062706270627064%, Average Validation Loss: 5.22485352664104\n",
            "EPOCH 6:\n",
            "Training: \n",
            " Training Accuracy: 37.95918367346939%, Average Training Loss: 5.137859772662727\n",
            "Validation: \n",
            " Validation Accuracy: 35.64356435643565%, Average Validation Loss: 5.154992547365699\n",
            "EPOCH 7:\n",
            "Training: \n",
            " Training Accuracy: 48.57142857142857%, Average Training Loss: 5.034354258556755\n",
            "Validation: \n",
            " Validation Accuracy: 45.54455445544554%, Average Validation Loss: 5.094613646516706\n",
            "EPOCH 8:\n",
            "Training: \n",
            " Training Accuracy: 58.16326530612245%, Average Training Loss: 4.936659559911611\n",
            "Validation: \n",
            " Validation Accuracy: 55.775577557755774%, Average Validation Loss: 5.033745535922916\n",
            "EPOCH 9:\n",
            "Training: \n",
            " Training Accuracy: 70.0%, Average Training Loss: 4.838179958109953\n",
            "Validation: \n",
            " Validation Accuracy: 61.71617161716171%, Average Validation Loss: 4.978998300659381\n",
            "EPOCH 10:\n",
            "Training: \n",
            " Training Accuracy: 77.55102040816327%, Average Training Loss: 4.7451714593537\n",
            "Validation: \n",
            " Validation Accuracy: 73.92739273927393%, Average Validation Loss: 4.8912248123597\n",
            "EPOCH 11:\n",
            "Training: \n",
            " Training Accuracy: 85.91836734693878%, Average Training Loss: 4.65560009041611\n",
            "Validation: \n",
            " Validation Accuracy: 79.20792079207921%, Average Validation Loss: 4.810454190760949\n",
            "EPOCH 12:\n",
            "Training: \n",
            " Training Accuracy: 89.79591836734694%, Average Training Loss: 4.585228190130117\n",
            "Validation: \n",
            " Validation Accuracy: 82.17821782178218%, Average Validation Loss: 4.774989065163993\n",
            "EPOCH 13:\n",
            "Training: \n",
            " Training Accuracy: 93.06122448979592%, Average Training Loss: 4.533614508959712\n",
            "Validation: \n",
            " Validation Accuracy: 86.46864686468646%, Average Validation Loss: 4.759381166779169\n",
            "EPOCH 14:\n",
            "Training: \n",
            " Training Accuracy: 94.6938775510204%, Average Training Loss: 4.50255790048716\n",
            "Validation: \n",
            " Validation Accuracy: 87.12871287128714%, Average Validation Loss: 4.724341762341289\n",
            "EPOCH 15:\n",
            "Training: \n",
            " Training Accuracy: 95.3061224489796%, Average Training Loss: 4.477197666557467\n",
            "Validation: \n",
            " Validation Accuracy: 88.11881188118812%, Average Validation Loss: 4.735320361927397\n",
            "EPOCH 16:\n",
            "Training: \n",
            " Training Accuracy: 96.73469387755102%, Average Training Loss: 4.463668219897212\n",
            "Validation: \n",
            " Validation Accuracy: 88.77887788778878%, Average Validation Loss: 4.657028218700548\n",
            "EPOCH 17:\n",
            "Training: \n",
            " Training Accuracy: 96.73469387755102%, Average Training Loss: 4.456519204743055\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.677171600140361\n",
            "EPOCH 18:\n",
            "Training: \n",
            " Training Accuracy: 97.55102040816327%, Average Training Loss: 4.446575836259491\n",
            "Validation: \n",
            " Validation Accuracy: 90.75907590759076%, Average Validation Loss: 4.644173590656948\n",
            "EPOCH 19:\n",
            "Training: \n",
            " Training Accuracy: 97.95918367346938%, Average Training Loss: 4.437654485507887\n",
            "Validation: \n",
            " Validation Accuracy: 89.43894389438944%, Average Validation Loss: 4.661486074869389\n",
            "EPOCH 20:\n",
            "Training: \n",
            " Training Accuracy: 98.57142857142857%, Average Training Loss: 4.435329573495047\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.6752462434296564\n",
            "EPOCH 21:\n",
            "Training: \n",
            " Training Accuracy: 98.77551020408163%, Average Training Loss: 4.429595139561867\n",
            "Validation: \n",
            " Validation Accuracy: 90.42904290429043%, Average Validation Loss: 4.639379580028773\n",
            "EPOCH 22:\n",
            "Training: \n",
            " Training Accuracy: 98.9795918367347%, Average Training Loss: 4.424916014379384\n",
            "Validation: \n",
            " Validation Accuracy: 91.08910891089108%, Average Validation Loss: 4.6197716159002225\n",
            "EPOCH 23:\n",
            "Training: \n",
            " Training Accuracy: 98.9795918367347%, Average Training Loss: 4.4224190906602505\n",
            "Validation: \n",
            " Validation Accuracy: 91.41914191419141%, Average Validation Loss: 4.624094228146493\n",
            "EPOCH 24:\n",
            "Training: \n",
            " Training Accuracy: 98.9795918367347%, Average Training Loss: 4.421707552306506\n",
            "Validation: \n",
            " Validation Accuracy: 90.75907590759076%, Average Validation Loss: 4.648183665259837\n",
            "EPOCH 25:\n",
            "Training: \n",
            " Training Accuracy: 98.9795918367347%, Average Training Loss: 4.420518943241665\n",
            "Validation: \n",
            " Validation Accuracy: 91.74917491749174%, Average Validation Loss: 4.628159990405092\n",
            "EPOCH 26:\n",
            "Training: \n",
            " Training Accuracy: 99.18367346938776%, Average Training Loss: 4.420856475830078\n",
            "Validation: \n",
            " Validation Accuracy: 91.74917491749174%, Average Validation Loss: 4.612297059679189\n",
            "EPOCH 27:\n",
            "Training: \n",
            " Training Accuracy: 99.18367346938776%, Average Training Loss: 4.418313911982945\n",
            "Validation: \n",
            " Validation Accuracy: 91.41914191419141%, Average Validation Loss: 4.611686862341248\n",
            "EPOCH 28:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.41785935966336\n",
            "Validation: \n",
            " Validation Accuracy: 92.73927392739274%, Average Validation Loss: 4.61941109238678\n",
            "EPOCH 29:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.415469558871522\n",
            "Validation: \n",
            " Validation Accuracy: 90.75907590759076%, Average Validation Loss: 4.615690343057362\n",
            "EPOCH 30:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.415189188353869\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.591275619594964\n",
            "EPOCH 31:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.414873911409962\n",
            "Validation: \n",
            " Validation Accuracy: 91.74917491749174%, Average Validation Loss: 4.598844473118042\n",
            "EPOCH 32:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.4149380995302785\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.604783018823504\n",
            "EPOCH 33:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.4146643560759875\n",
            "Validation: \n",
            " Validation Accuracy: 91.74917491749174%, Average Validation Loss: 4.601451271044539\n",
            "EPOCH 34:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.414160971738855\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.597132657620773\n",
            "EPOCH 35:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.414596119705512\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.577878981927047\n",
            "EPOCH 36:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.414227252103845\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.597907946054692\n",
            "EPOCH 37:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.413832635295634\n",
            "Validation: \n",
            " Validation Accuracy: 90.75907590759076%, Average Validation Loss: 4.6028985945698455\n",
            "EPOCH 38:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.413960865565708\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.598429136937208\n",
            "EPOCH 39:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.413631030491421\n",
            "Validation: \n",
            " Validation Accuracy: 91.74917491749174%, Average Validation Loss: 4.588124397957679\n",
            "EPOCH 40:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.41356744571608\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.604415764509648\n",
            "EPOCH 41:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.413551680895747\n",
            "Validation: \n",
            " Validation Accuracy: 91.41914191419141%, Average Validation Loss: 4.584944427603542\n",
            "EPOCH 42:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.413396815864408\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.580646247360179\n",
            "EPOCH 43:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.413490587351274\n",
            "Validation: \n",
            " Validation Accuracy: 91.74917491749174%, Average Validation Loss: 4.58897340258356\n",
            "EPOCH 44:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.413308240929428\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.581914075530402\n",
            "EPOCH 45:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.41327377241485\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.598046858318568\n",
            "EPOCH 46:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.41306294227133\n",
            "Validation: \n",
            " Validation Accuracy: 93.3993399339934%, Average Validation Loss: 4.585639911122842\n",
            "EPOCH 47:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.4130725763282\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.579635461171468\n",
            "EPOCH 48:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412972790854318\n",
            "Validation: \n",
            " Validation Accuracy: 93.72937293729373%, Average Validation Loss: 4.582409251247696\n",
            "EPOCH 49:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.4130352857161546\n",
            "Validation: \n",
            " Validation Accuracy: 91.74917491749174%, Average Validation Loss: 4.589252890533347\n",
            "EPOCH 50:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.413222488091916\n",
            "Validation: \n",
            " Validation Accuracy: 91.41914191419141%, Average Validation Loss: 4.588415325278103\n",
            "EPOCH 51:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.41300969221154\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.584041357827266\n",
            "EPOCH 52:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.413007171786561\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.566314780672784\n",
            "EPOCH 53:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412850000420395\n",
            "Validation: \n",
            " Validation Accuracy: 90.0990099009901%, Average Validation Loss: 4.586408294073426\n",
            "EPOCH 54:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412736308817961\n",
            "Validation: \n",
            " Validation Accuracy: 91.41914191419141%, Average Validation Loss: 4.571844134000268\n",
            "EPOCH 55:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412820417053846\n",
            "Validation: \n",
            " Validation Accuracy: 91.74917491749174%, Average Validation Loss: 4.582755383091791\n",
            "EPOCH 56:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412754253465302\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.5771061201693595\n",
            "EPOCH 57:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412742760716652\n",
            "Validation: \n",
            " Validation Accuracy: 91.08910891089108%, Average Validation Loss: 4.5884291142126905\n",
            "EPOCH 58:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412784712655204\n",
            "Validation: \n",
            " Validation Accuracy: 91.41914191419141%, Average Validation Loss: 4.569209574079356\n",
            "EPOCH 59:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412638810216164\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.573151679715701\n",
            "EPOCH 60:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.41262099207664\n",
            "Validation: \n",
            " Validation Accuracy: 91.41914191419141%, Average Validation Loss: 4.575891990472774\n",
            "EPOCH 61:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412571118802441\n",
            "Validation: \n",
            " Validation Accuracy: 90.42904290429043%, Average Validation Loss: 4.57581911779473\n",
            "EPOCH 62:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412562234061105\n",
            "Validation: \n",
            " Validation Accuracy: 90.75907590759076%, Average Validation Loss: 4.572909353589854\n",
            "EPOCH 63:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412536572436897\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.586249878697663\n",
            "EPOCH 64:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.4125984444910165\n",
            "Validation: \n",
            " Validation Accuracy: 92.73927392739274%, Average Validation Loss: 4.572341545973674\n",
            "EPOCH 65:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412577687477579\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.561006235997669\n",
            "EPOCH 66:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412481755626445\n",
            "Validation: \n",
            " Validation Accuracy: 91.41914191419141%, Average Validation Loss: 4.597583761309633\n",
            "EPOCH 67:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412552677855199\n",
            "Validation: \n",
            " Validation Accuracy: 91.74917491749174%, Average Validation Loss: 4.573634476551522\n",
            "EPOCH 68:\n",
            "Training: \n",
            " Training Accuracy: 99.38775510204081%, Average Training Loss: 4.412423737195073\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.57430881084782\n",
            "EPOCH 69:\n",
            "Training: \n",
            " Training Accuracy: 99.18367346938776%, Average Training Loss: 4.41606836902852\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.5672956866399685\n",
            "EPOCH 70:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410902976989746\n",
            "Validation: \n",
            " Validation Accuracy: 91.41914191419141%, Average Validation Loss: 4.581462841222782\n",
            "EPOCH 71:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410498696930555\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.56160582174169\n",
            "EPOCH 72:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4112714456052196\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.583048324773808\n",
            "EPOCH 73:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410584041050503\n",
            "Validation: \n",
            " Validation Accuracy: 93.06930693069307%, Average Validation Loss: 4.56251332468719\n",
            "EPOCH 74:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4104538450435715\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.559025326971174\n",
            "EPOCH 75:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4103400074705785\n",
            "Validation: \n",
            " Validation Accuracy: 91.74917491749174%, Average Validation Loss: 4.588900970547113\n",
            "EPOCH 76:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4102984545182204\n",
            "Validation: \n",
            " Validation Accuracy: 94.05940594059406%, Average Validation Loss: 4.5579942177624595\n",
            "EPOCH 77:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410344561752008\n",
            "Validation: \n",
            " Validation Accuracy: 92.73927392739274%, Average Validation Loss: 4.555127362606942\n",
            "EPOCH 78:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410337808180828\n",
            "Validation: \n",
            " Validation Accuracy: 91.08910891089108%, Average Validation Loss: 4.576524048355153\n",
            "EPOCH 79:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410301043062794\n",
            "Validation: \n",
            " Validation Accuracy: 92.73927392739274%, Average Validation Loss: 4.547963915091537\n",
            "EPOCH 80:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410249885247678\n",
            "Validation: \n",
            " Validation Accuracy: 93.3993399339934%, Average Validation Loss: 4.553192596624394\n",
            "EPOCH 81:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4102373415110065\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.551398440949594\n",
            "EPOCH 82:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410236699240548\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.548104423107487\n",
            "EPOCH 83:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.41024535042899\n",
            "Validation: \n",
            " Validation Accuracy: 94.05940594059406%, Average Validation Loss: 4.5461345112363105\n",
            "EPOCH 84:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4102243793254\n",
            "Validation: \n",
            " Validation Accuracy: 93.06930693069307%, Average Validation Loss: 4.548832767473982\n",
            "EPOCH 85:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410229137965611\n",
            "Validation: \n",
            " Validation Accuracy: 93.06930693069307%, Average Validation Loss: 4.543513729233946\n",
            "EPOCH 86:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410192110100571\n",
            "Validation: \n",
            " Validation Accuracy: 92.73927392739274%, Average Validation Loss: 4.550919403730839\n",
            "EPOCH 87:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410206872589734\n",
            "Validation: \n",
            " Validation Accuracy: 91.74917491749174%, Average Validation Loss: 4.562025827149747\n",
            "EPOCH 88:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410176335548868\n",
            "Validation: \n",
            " Validation Accuracy: 92.73927392739274%, Average Validation Loss: 4.542940690572506\n",
            "EPOCH 89:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.41020899402852\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.548389751132172\n",
            "EPOCH 90:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4101748077236875\n",
            "Validation: \n",
            " Validation Accuracy: 92.73927392739274%, Average Validation Loss: 4.5487421920197235\n",
            "EPOCH 91:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410167178329156\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.554551184374113\n",
            "EPOCH 92:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410179877767757\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.551078969495918\n",
            "EPOCH 93:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.41017264735942\n",
            "Validation: \n",
            " Validation Accuracy: 92.40924092409242%, Average Validation Loss: 4.547942348832738\n",
            "EPOCH 94:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410165416951082\n",
            "Validation: \n",
            " Validation Accuracy: 91.74917491749174%, Average Validation Loss: 4.556757502036519\n",
            "EPOCH 95:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410188188358229\n",
            "Validation: \n",
            " Validation Accuracy: 93.06930693069307%, Average Validation Loss: 4.542549690397659\n",
            "EPOCH 96:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410154371845479\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.545368644663997\n",
            "EPOCH 97:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410156395970558\n",
            "Validation: \n",
            " Validation Accuracy: 91.41914191419141%, Average Validation Loss: 4.5595301983773515\n",
            "EPOCH 98:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.4101371083940775\n",
            "Validation: \n",
            " Validation Accuracy: 92.07920792079207%, Average Validation Loss: 4.540682614833215\n",
            "EPOCH 99:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410137517111642\n",
            "Validation: \n",
            " Validation Accuracy: 92.73927392739274%, Average Validation Loss: 4.541099853641523\n",
            "EPOCH 100:\n",
            "Training: \n",
            " Training Accuracy: 99.59183673469387%, Average Training Loss: 4.410147345795924\n",
            "Validation: \n",
            " Validation Accuracy: 92.73927392739274%, Average Validation Loss: 4.5450590467295635\n"
          ]
        }
      ]
    }
  ]
}